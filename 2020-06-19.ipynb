{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### past\n",
    "- notebook \"2020-06-14.ipynb\", tested the capability to continue fitting `clf.fit(X[part], y_enc[part], xgb_model=prev_model)` and saving `clf.save_model(prev_model)` a model in a loop, over parts of a training set. \n",
    "- however, the `predict_proba()` step on the test set at each iteration, was taking a very long time. Somtimes half an hour or 40 minutes. \n",
    "- In notebook \"2020-06-16.ipynb\" , I switrched to evaluating on that test set every epoch instead of every batch. Still, this finished `14` epochs in `13` hours and I even cut my data set by `60x` . That also appears to have deteriorated the test accuracy from `acc=0.12` in \"2020-06-14.ipynb\" to `acc=0.021` in \"2020-06-16.ipynb\". Again the train and test sets were cut by `60x` so that definitely changed things. But also the `14` epochs did not appear to be helping. \n",
    "\n",
    "\n",
    "#### other notes\n",
    "- feels like better balanced datasets are important . Perhaps otherwise the time to use them is maybe excessively long.\n",
    "- predict proba is slow ( as per my [2020-06-14 logs here](https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-06-14.md#output-logs) )\n",
    "- I still feel like `100` iterations feels like a minimum of sorts to aim for.\n",
    "- got to re-do 3 or 5 fold cross validation now that weird log loss is good.\n",
    "- If I do the non-sklearn xgboost, I can take advantage of that dataset caching, which I think saved on memory.\n",
    "- So perhaps one goal is to try that non-sklearn xgboost, w/ `100` iterations, but using at least those parameters from bundle `'/opt/program/artifacts/2020-06-11T041641Z/bundle.joblib'` which appeared to be a good start..\n",
    "\n",
    "```\n",
    "\n",
    "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "               colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "               learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "               min_child_weight=1, missing=nan, n_estimators=100, n_jobs=1,\n",
    "               nthread=None, objective='multi:softprob', random_state=0,\n",
    "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "               silent=None, subsample=1, verbosity=1),\n",
    "               \n",
    "```\n",
    "\n",
    "#### Are those iterations \"built in\" ? \n",
    "- If I am \"manually\" iterating then am I actually doing 100*100 = 10,000 iterations?! \n",
    "- Perhaps I can check by using the verbose mode\n",
    "\n",
    "#### Memory consumed by other notebooks\n",
    "- also important note that I forgot that other notebooks may have been sitting on memory, slowing down the notebook where I was actually running work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND\r\n",
      "/opt/conda/bin/python -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5732db1a-d484-4a58-9d67-de6ef5ac721b.json\r\n"
     ]
    }
   ],
   "source": [
    "#!ps -p 689 -o pid,ppid,pmem,rss,res\n",
    "!foo=$(ps -p 689 -o command); echo \"$foo\"\n",
    "#!ps -p 689 -o command\n",
    "# 727\n",
    "# COMMAND /opt/conda/bin/python -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-11cd26cf-5715-4e84-abd4-ee49b0b06ed9.json\n",
    "\n",
    "# 689\n",
    "# COMMAND /opt/conda/bin/python -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5732db1a-d484-4a58-9d67-de6ef5ac721b.json\n",
    "#!cat /root/.local/share/jupyter/runtime/kernel-5732db1a-d484-4a58-9d67-de6ef5ac721b.json\n",
    "#os.getpid()\n",
    "\n",
    "# 667 , the parent\n",
    "# COMMAND /opt/conda/bin/python /opt/conda/bin/jupyter-notebook --ip 0.0.0.0 --port 8889 --no-browser --allow-root\n",
    "#!foo=$(ps auxww); echo \"$foo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=\u001b[H\u001b[2J\u001b[mtop - 12:25:39 up 4 days,  6:45,  0 users,  load average: 0.00, 0.02, 0.00\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "Tasks:\u001b[m\u001b[m\u001b[1m  10 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mrunning,\u001b[m\u001b[m\u001b[1m   9 \u001b[m\u001b[msleeping,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mstopped,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mzombie\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "%Cpu(s):\u001b[m\u001b[m\u001b[1m  6.7 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.7 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 92.3 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.2 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.1 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Mem :\u001b[m\u001b[m\u001b[1m  2046844 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m    82660 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m  1855176 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m   109008 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Swap:\u001b[m\u001b[m\u001b[1m  1048572 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   695520 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m   353052 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m    50424 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "\u001b[K\n",
      "\u001b[7m  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND     \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m    1 root      20   0   18236    204    100 S   0.0  0.0   0:00.11 bash        \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  667 root      20   0  999296  56116   5060 S   0.0  2.7   0:21.62 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  673 root      20   0  555440   8460   3128 S   0.0  0.4   0:24.88 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  689 root      20   0 2521820 1.492g  18052 S   0.0 76.4 760:54.45 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  711 root      20   0  555292  44516   6204 S   0.0  2.2   0:15.04 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  727 root      20   0  555292  44656   6280 S   0.0  2.2   0:15.52 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  743 root      20   0  555292  44644   6316 S   0.0  2.2   0:14.36 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  759 root      20   0  562420  53140  13436 S   0.0  2.6   0:02.17 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  783 root      20   0    4500    672    600 S   0.0  0.0   0:00.53 sh          \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m\u001b[1m  784 root      20   0   36856   3004   2592 R   0.0  0.1   0:00.00 top         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[J\u001b[H\u001b[mtop - 12:25:42 up 4 days,  6:45,  0 users,  load average: 0.00, 0.02, 0.00\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "Tasks:\u001b[m\u001b[m\u001b[1m  10 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mrunning,\u001b[m\u001b[m\u001b[1m   9 \u001b[m\u001b[msleeping,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mstopped,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mzombie\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.4 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.5 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.1 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Mem :\u001b[m\u001b[m\u001b[1m  2046844 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m    82660 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m  1855168 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m   109016 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "\n",
      "\u001b[K\n",
      "\n",
      "\u001b[m  759 root      20   0  562420  53140  13436 S   0.3  2.6   0:02.18 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m    1 root      20   0   18236    204    100 S   0.0  0.0   0:00.11 bash        \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  667 root      20   0  999296  56116   5060 S   0.0  2.7   0:21.62 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  673 root      20   0  555440   8460   3128 S   0.0  0.4   0:24.88 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  689 root      20   0 2521820 1.492g  18052 S   0.0 76.4 760:54.45 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  711 root      20   0  555292  44516   6204 S   0.0  2.2   0:15.04 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  727 root      20   0  555292  44656   6280 S   0.0  2.2   0:15.52 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  743 root      20   0  555292  44644   6316 S   0.0  2.2   0:14.36 python      \u001b[m\u001b[m\u001b[K\n",
      "\n",
      "\u001b[m\u001b[1m  784 root      20   0   36824   3028   2592 R   0.0  0.1   0:00.00 top         \u001b[m\u001b[m\u001b[K\n",
      "\u001b[J\u001b[H\u001b[mtop - 12:25:45 up 4 days,  6:45,  0 users,  load average: 0.00, 0.02, 0.00\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "\n",
      "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.7 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.3 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.1 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "KiB Mem :\u001b[m\u001b[m\u001b[1m  2046844 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m    82660 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m  1855164 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m   109020 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "\n",
      "\u001b[K\n",
      "\n",
      "\u001b[m  667 root      20   0  999296  56116   5060 S   0.3  2.7   0:21.63 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
      "\n",
      "\u001b[m  673 root      20   0  555440   8460   3128 S   0.0  0.4   0:24.88 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  689 root      20   0 2521820 1.492g  18052 S   0.0 76.4 760:54.45 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  711 root      20   0  555292  44516   6204 S   0.0  2.2   0:15.04 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  727 root      20   0  555292  44656   6280 S   0.0  2.2   0:15.52 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  743 root      20   0  555292  44644   6316 S   0.0  2.2   0:14.36 python      \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  759 root      20   0  562420  53140  13436 S   0.0  2.6   0:02.18 python      \u001b[m\u001b[m\u001b[K\n",
      "\n",
      "\n",
      "\u001b[J\u001b[H\u001b[mtop - 12:25:48 up 4 days,  6:45,  0 users,  load average: 0.00, 0.02, 0.00\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "\n",
      "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.7 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.6 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 98.7 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
      "\n",
      "\n",
      "\u001b[K\n",
      "\n",
      "\u001b[m    1 root      20   0   18236    204    100 S   0.0  0.0   0:00.11 bash        \u001b[m\u001b[m\u001b[K\n",
      "\u001b[m  667 root      20   0  999296  56116   5060 S   0.0  2.7   0:21.63 jupyter-no+ \u001b[m\u001b[m\u001b[K\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[J\u001b[?1l\u001b>\u001b[25;1H\n",
      "\u001b[K"
     ]
    }
   ],
   "source": [
    "!top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look how class-balanced the data is... and given some data, \"carve\" a balanced training set out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import datetime; import pytz\n",
    "import matplotlib as plt\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split # (*arrays, **options)\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from joblib import dump, load\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import fresh.utils as fu\n",
    "\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import fresh.preproc.v1 as pv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/opt/data'\n",
    "localdir = '/opt/program'\n",
    "\n",
    "\n",
    "tripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv'\n",
    "                     ) # .sample(frac=0.017, random_state=42)\n",
    "stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',\n",
    "                        index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# preproc\n",
    "(X_transformed,\n",
    "     one_hot_enc, le,\n",
    "     y_enc) = pv1.preprocess(X_train, y_train, # X[train_index]\n",
    "                         neighborhoods)\n",
    "labels = le.classes_\n",
    "\n",
    "# Test set\n",
    "X_test_transformed = one_hot_enc.transform(X_test)\n",
    "y_test_enc = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module xgboost.sklearn:\n",
      "\n",
      "fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None) method of xgboost.sklearn.XGBClassifier instance\n",
      "    Fit gradient boosting classifier\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array_like\n",
      "        Feature matrix\n",
      "    y : array_like\n",
      "        Labels\n",
      "    sample_weight : array_like\n",
      "        Weight for each instance\n",
      "    eval_set : list, optional\n",
      "        A list of (X, y) pairs to use as a validation set for\n",
      "        early-stopping\n",
      "    sample_weight_eval_set : list, optional\n",
      "        A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "        instance weights on the i-th validation set.\n",
      "    eval_metric : str, callable, optional\n",
      "        If a str, should be a built-in evaluation metric to use. See\n",
      "        doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "        signature is func(y_predicted, y_true) where y_true will be a\n",
      "        DMatrix object such that you may need to call the get_label\n",
      "        method. It must return a str, value pair where the str is a name\n",
      "        for the evaluation and value is the value of the evaluation\n",
      "        function. This objective is always minimized.\n",
      "    early_stopping_rounds : int, optional\n",
      "        Activates early stopping. Validation error needs to decrease at\n",
      "        least every <early_stopping_rounds> round(s) to continue training.\n",
      "        Requires at least one item in evals. If there's more than one,\n",
      "        will use the last. If early stopping occurs, the model will have\n",
      "        three additional fields: bst.best_score, bst.best_iteration and\n",
      "        bst.best_ntree_limit (bst.best_ntree_limit is the ntree_limit parameter\n",
      "        default value in predict method if not any other value is specified).\n",
      "        (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "        and/or num_class appears in the parameters)\n",
      "    verbose : bool\n",
      "        If `verbose` and an evaluation set is used, writes the evaluation\n",
      "        metric measured on the validation set to stderr.\n",
      "    xgb_model : str\n",
      "        file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "        loaded before training (allows training continuation).\n",
      "    callbacks : list of callback functions\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "        Example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.XGBClassifier().fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'East Village': 227,\n",
       "         'Williamsburg': 189,\n",
       "         'Midtown East': 717,\n",
       "         'Chelsea': 1584,\n",
       "         'Bowery': 271,\n",
       "         'Battery Park City': 433,\n",
       "         'Nolita': 183,\n",
       "         'Tribeca': 662,\n",
       "         'Financial District': 627,\n",
       "         'Lower East Side': 452,\n",
       "         'Gramercy Park': 240,\n",
       "         'Ukrainian Village': 373,\n",
       "         'Alphabet City': 356,\n",
       "         'NoHo': 67,\n",
       "         'Civic Center': 206,\n",
       "         'Murray Hill': 285,\n",
       "         'Dumbo': 105,\n",
       "         'Flatiron District': 256,\n",
       "         'Greenwich Village': 689,\n",
       "         'Garment District': 235,\n",
       "         'Meatpacking District': 127,\n",
       "         'Stuyvesant Town': 202,\n",
       "         'Rose Hill': 129,\n",
       "         \"Hell's Kitchen\": 601,\n",
       "         'Kips Bay': 411,\n",
       "         'Lincoln Square': 151,\n",
       "         'Midtown': 444,\n",
       "         'Midtown West': 565,\n",
       "         'Hudson Square': 167,\n",
       "         'West Village': 619,\n",
       "         'Theater District': 374,\n",
       "         'Lower Manhattan': 233,\n",
       "         'Union Square': 201,\n",
       "         'SoHo': 234,\n",
       "         'NoMad': 101,\n",
       "         'Bedford-Stuyvesant': 67,\n",
       "         'Downtown Brooklyn': 207,\n",
       "         'Chinatown': 215,\n",
       "         'Little Italy': 45,\n",
       "         'Fort Greene': 274,\n",
       "         'Central Park': 94,\n",
       "         'Brooklyn Navy Yard': 45,\n",
       "         'Peter Cooper Village': 55,\n",
       "         'Boerum Hill': 53,\n",
       "         'Brooklyn Heights': 142,\n",
       "         'Clinton Hill': 124,\n",
       "         'Fulton Ferry District': 30,\n",
       "         'Tudor City': 20,\n",
       "         'Korea Town': 60,\n",
       "         'Two Bridges': 97,\n",
       "         'Park Slope': 37,\n",
       "         'Columbia Street Waterfront District': 37,\n",
       "         'Vinegar Hill': 9,\n",
       "         'Sutton Place': 11})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "workdir = fu.make_work_dir(); print(workdir)\n",
    "fu.log(workdir, 'Starting')\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    # preproc\n",
    "    (X_transformed,\n",
    "         one_hot_enc, le,\n",
    "         y_enc) = preprocess(X[train_index], y[train_index], \n",
    "                             neighborhoods)\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier().fit(X_transformed, \n",
    "                                        y_enc, verbose=True)\n",
    "    #\n",
    "    X_test_transformed = one_hot_enc.transform(X[test_index])\n",
    "    actuals = le.transform(y[test_index])\n",
    "    \n",
    "    predictions = xgb_model.predict(X_test_transformed)\n",
    "    print(confusion_matrix(actuals, predictions))\n",
    "    \n",
    "    preds = xgb_model.predict_proba(X_test_transformed)\n",
    "    \n",
    "    \n",
    "    loss = fu.big_logloss(y_test_enc, y_prob_vec, list(range(len(labels)))\n",
    "    fu.log(workdir, f'[{epoch}] Done big_logloss, loss={loss}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (utils.py, line 90)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3331\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-82-e274d1900c73>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    reload(fu)\n",
      "  File \u001b[1;32m\"/opt/conda/lib/python3.7/importlib/__init__.py\"\u001b[0m, line \u001b[1;32m169\u001b[0m, in \u001b[1;35mreload\u001b[0m\n    _bootstrap._exec(spec, module)\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m630\u001b[0m, in \u001b[1;35m_exec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m724\u001b[0m, in \u001b[1;35mexec_module\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m860\u001b[0m, in \u001b[1;35mget_code\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m791\u001b[0m, in \u001b[1;35msource_to_code\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<frozen importlib._bootstrap>\"\u001b[0;36m, line \u001b[0;32m219\u001b[0;36m, in \u001b[0;35m_call_with_frames_removed\u001b[0;36m\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/opt/program/fresh/utils.py\"\u001b[0;36m, line \u001b[0;32m90\u001b[0m\n\u001b[0;31m    slices =\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "reload(fu)\n",
    "print(fu.get_slices(range(1000), slice_size=444))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 ms, sys: 0 ns, total: 10 ms\n",
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 4, 12, 24, 40, 60, 84, 112, 144, 180]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(joblib)\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from math import sqrt\n",
    "\n",
    "def mywork(x, y, z=None, a=None):\n",
    "    time.sleep(5)\n",
    "    \n",
    "    return x*y * (z or a)\n",
    "\n",
    "%time Parallel(n_jobs=5)(delayed(mywork)(i, i+1, z=2) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fresh.utils' from '/opt/program/fresh/utils.py'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(fu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 660 ms, sys: 50 ms, total: 710 ms\n",
      "Wall time: 725 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_prob_vec = np.random.random(size=(y_test_enc.shape[0],\n",
    "                       labels.shape[0]))\n",
    "loss = fu.big_logloss(y_test_enc, y_prob_vec, list(range(len(labels))), parallel=False)\n",
    "# Wall time: 56.5 s\n",
    "# Wall time: 906 ms  . huh?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss#labels#y_test_enc.shape, y_prob_vec.shape#loss # 4.291267319642341\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210854,), (54,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21080369, 0.43961201, 0.19590716, ..., 0.71289874, 0.33170714,\n",
       "        0.82601838],\n",
       "       [0.36605098, 0.13483735, 0.90106828, ..., 0.54691864, 0.4643664 ,\n",
       "        0.88128189],\n",
       "       [0.63243054, 0.16359919, 0.66125254, ..., 0.34549834, 0.41448938,\n",
       "        0.6333276 ],\n",
       "       ...,\n",
       "       [0.14012141, 0.33125609, 0.49782631, ..., 0.26443373, 0.86010273,\n",
       "        0.96617081],\n",
       "       [0.50237769, 0.86188519, 0.61262241, ..., 0.67107822, 0.36280706,\n",
       "        0.08816574],\n",
       "       [0.02359937, 0.74809099, 0.15765813, ..., 0.00790811, 0.13526414,\n",
       "        0.87367221]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(size=(y_test_enc.shape[0],\n",
    "                       labels.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
