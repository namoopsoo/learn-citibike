{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to end of 2020-06-14 , but I want to try many iterations, still batching to try avoid \n",
    "kernel exploding\n",
    "\n",
    "- This time... going to just perhaps, shrink the test set and only test after every iteration..\n",
    "\n",
    "- In the 2020-06-14 notebook, ran through 17 out of the 633 or so batches I had setup per epoch. And that took the whole night when I slept  (2020-06-16 03:57:33Z to  16:20:23Z  ) \n",
    "- During that time, the accuracy did not budge really. (The logloss was 0 for some reason. I still have not debugged that ). Got to debug that 0 logloss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import datetime; import pytz\n",
    "import matplotlib as plt\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split # (*arrays, **options)\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from joblib import dump, load\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import fresh.utils as fu\n",
    "\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import fresh.preproc.v1 as pv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/opt/data'\n",
    "localdir = '/opt/program'\n",
    "\n",
    "# This time only take about a 1/60th  of the data... 0.017% ... \n",
    "tripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv'\n",
    "                     ).sample(frac=0.017, random_state=42)\n",
    "stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',\n",
    "                        index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# preproc\n",
    "(X_transformed,\n",
    "     one_hot_enc, le,\n",
    "     y_enc) = pv1.preprocess(X_train, y_train, # X[train_index]\n",
    "                         neighborhoods)\n",
    "labels = le.classes_\n",
    "\n",
    "# Test set\n",
    "X_test_transformed = one_hot_enc.transform(X_test)\n",
    "y_test_enc = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "size = X_transformed.shape[0]\n",
    "print(size)\n",
    "indices = np.random.choice(range(size), size=size, replace=False)\n",
    "\n",
    "parts = fu.get_partitions(indices, slice_size=1000)\n",
    "len(parts[0])\n",
    "len(Counter(y_enc[parts[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3585,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time... going to just perhaps, shrink the test set and only test after every iteration.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/program/artifacts/2020-06-17T042025Z\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90cb0a51f944f3d99cf86d0d290ae08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa7d9e9d9434f608047b0d9ebe258c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0c7b14cbe3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'[{epoch}:{i}] Done fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batches'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mfu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'[{epoch}:{i}] Done fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "workdir = fu.make_work_dir(); print(workdir)\n",
    "fu.log(workdir, 'Starting')\n",
    "prev_model = None\n",
    "loss_vec = []; acc_vec = []\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "for epoch in tqdm(range(100), desc='epochs'):\n",
    "    fu.log(workdir, f'[{epoch}:{i}] Done fit')\n",
    "    for i, part in enumerate(tqdm(parts, leave=False, desc='batches')):\n",
    "        clf.fit(X_transformed[part], y_enc[part], xgb_model=prev_model)\n",
    "        fu.log(workdir, f'[{epoch}:{i}] Done fit')\n",
    "\n",
    "        prev_model = f'{workdir}/model.xg'\n",
    "        clf.save_model(prev_model)\n",
    "\n",
    "    y_prob_vec = clf.predict_proba(X_test_transformed)\n",
    "    fu.log(workdir, f'[{epoch}] Done predict_proba')\n",
    "\n",
    "    loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)\n",
    "    fu.log(workdir, f'[{epoch}] Done big_logloss, loss={loss}.')\n",
    "\n",
    "    loss_vec.append(loss)\n",
    "\n",
    "    acc = accuracy_score(y_test_enc, np.argmax(y_prob_vec, axis=1))\n",
    "    acc_vec.append(acc)\n",
    "    fu.log(workdir, f'[{epoch}] Done accuracy, acc={acc}.')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As this is running, I'm definitely wondering if different forms of batching deteriorates the ability to learn.\n",
    "- Or perhaps is that just a parameter tuning problem?\n",
    "\n",
    "- I killed this loop because not much progress was made after `13` iterations\n",
    "\n",
    "```\n",
    "(pandars3) $ tail -f artifacts/2020-06-17T042025Z/work.log \n",
    "2020-06-17 04:20:25Z, Starting\n",
    "2020-06-17 04:20:25Z, [0:3] Done fit\n",
    "2020-06-17 04:20:27Z, [0:0] Done fit\n",
    "2020-06-17 04:20:30Z, [0:1] Done fit\n",
    "2020-06-17 04:20:34Z, [0:2] Done fit\n",
    "2020-06-17 04:20:40Z, [0:3] Done fit\n",
    "2020-06-17 04:20:51Z, [0:4] Done fit\n",
    "2020-06-17 04:21:04Z, [0:5] Done fit\n",
    "2020-06-17 04:21:20Z, [0:6] Done fit\n",
    "2020-06-17 04:21:39Z, [0:7] Done fit\n",
    "2020-06-17 04:22:02Z, [0:8] Done fit\n",
    "2020-06-17 04:22:30Z, [0:9] Done fit\n",
    "2020-06-17 04:22:53Z, [0:10] Done fit\n",
    "2020-06-17 04:23:47Z, [0] Done predict_proba\n",
    "2020-06-17 04:23:47Z, [0] Done big_logloss, loss=0.0.\n",
    "2020-06-17 04:23:47Z, [0] Done accuracy, acc=0.023709902370990237.\n",
    "2020-06-17 04:23:47Z, [1:10] Done fit\n",
    "2020-06-17 04:24:17Z, [1:0] Done fit\n",
    "2020-06-17 04:24:52Z, [1:1] Done fit\n",
    "2020-06-17 04:25:30Z, [1:2] Done fit\n",
    "2020-06-17 04:26:12Z, [1:3] Done fit\n",
    "2020-06-17 04:26:56Z, [1:4] Done fit\n",
    "2020-06-17 04:27:44Z, [1:5] Done fit\n",
    "2020-06-17 04:28:35Z, [1:6] Done fit\n",
    "2020-06-17 04:29:30Z, [1:7] Done fit\n",
    "2020-06-17 04:30:30Z, [1:8] Done fit\n",
    "2020-06-17 04:31:31Z, [1:9] Done fit\n",
    "2020-06-17 04:32:22Z, [1:10] Done fit\n",
    "2020-06-17 04:34:16Z, [1] Done predict_proba\n",
    "2020-06-17 04:34:16Z, [1] Done big_logloss, loss=0.0.\n",
    "2020-06-17 04:34:16Z, [1] Done accuracy, acc=0.021757322175732216.\n",
    "2020-06-17 04:34:16Z, [2:10] Done fit\n",
    "2020-06-17 04:35:21Z, [2:0] Done fit\n",
    "2020-06-17 04:36:32Z, [2:1] Done fit\n",
    "2020-06-17 04:37:47Z, [2:2] Done fit\n",
    "2020-06-17 04:39:05Z, [2:3] Done fit\n",
    "2020-06-17 04:40:26Z, [2:4] Done fit\n",
    "2020-06-17 04:41:50Z, [2:5] Done fit\n",
    "2020-06-17 04:43:18Z, [2:6] Done fit\n",
    "2020-06-17 04:44:49Z, [2:7] Done fit\n",
    "2020-06-17 04:46:23Z, [2:8] Done fit\n",
    "2020-06-17 04:48:01Z, [2:9] Done fit\n",
    "2020-06-17 04:49:18Z, [2:10] Done fit\n",
    "2020-06-17 04:52:11Z, [2] Done predict_proba\n",
    "2020-06-17 04:52:11Z, [2] Done big_logloss, loss=0.0.\n",
    "2020-06-17 04:52:11Z, [2] Done accuracy, acc=0.017573221757322177.\n",
    "2020-06-17 04:52:11Z, [3:10] Done fit\n",
    "2020-06-17 04:53:48Z, [3:0] Done fit\n",
    "2020-06-17 04:55:33Z, [3:1] Done fit\n",
    "2020-06-17 04:57:22Z, [3:2] Done fit\n",
    "2020-06-17 04:59:12Z, [3:3] Done fit\n",
    "2020-06-17 05:01:06Z, [3:4] Done fit\n",
    "2020-06-17 05:03:03Z, [3:5] Done fit\n",
    "2020-06-17 05:05:05Z, [3:6] Done fit\n",
    "2020-06-17 05:07:07Z, [3:7] Done fit\n",
    "2020-06-17 05:09:15Z, [3:8] Done fit\n",
    "2020-06-17 05:11:26Z, [3:9] Done fit\n",
    "2020-06-17 05:13:10Z, [3:10] Done fit\n",
    "2020-06-17 05:17:04Z, [3] Done predict_proba\n",
    "2020-06-17 05:17:04Z, [3] Done big_logloss, loss=0.0.\n",
    "2020-06-17 05:17:04Z, [3] Done accuracy, acc=0.01701534170153417.\n",
    "2020-06-17 05:17:04Z, [4:10] Done fit\n",
    "2020-06-17 05:19:14Z, [4:0] Done fit\n",
    "2020-06-17 05:21:35Z, [4:1] Done fit\n",
    "2020-06-17 05:23:59Z, [4:2] Done fit\n",
    "2020-06-17 05:26:27Z, [4:3] Done fit\n",
    "2020-06-17 05:28:57Z, [4:4] Done fit\n",
    "2020-06-17 05:31:33Z, [4:5] Done fit\n",
    "2020-06-17 05:34:11Z, [4:6] Done fit\n",
    "2020-06-17 05:36:55Z, [4:7] Done fit\n",
    "2020-06-17 05:39:37Z, [4:8] Done fit\n",
    "2020-06-17 05:42:26Z, [4:9] Done fit\n",
    "2020-06-17 05:44:36Z, [4:10] Done fit\n",
    "2020-06-17 05:49:28Z, [4] Done predict_proba\n",
    "2020-06-17 05:49:28Z, [4] Done big_logloss, loss=0.0.\n",
    "2020-06-17 05:49:28Z, [4] Done accuracy, acc=0.01785216178521618.\n",
    "2020-06-17 05:49:28Z, [5:10] Done fit\n",
    "2020-06-17 05:52:17Z, [5:0] Done fit\n",
    "2020-06-17 05:55:13Z, [5:1] Done fit\n",
    "2020-06-17 05:58:15Z, [5:2] Done fit\n",
    "2020-06-17 06:01:17Z, [5:3] Done fit\n",
    "2020-06-17 06:04:28Z, [5:4] Done fit\n",
    "2020-06-17 06:07:37Z, [5:5] Done fit\n",
    "2020-06-17 06:10:55Z, [5:6] Done fit\n",
    "2020-06-17 06:14:13Z, [5:7] Done fit\n",
    "2020-06-17 06:17:36Z, [5:8] Done fit\n",
    "2020-06-17 06:21:02Z, [5:9] Done fit\n",
    "2020-06-17 06:23:46Z, [5:10] Done fit\n",
    "2020-06-17 06:29:54Z, [5] Done predict_proba\n",
    "2020-06-17 06:29:54Z, [5] Done big_logloss, loss=0.0.\n",
    "2020-06-17 06:29:54Z, [5] Done accuracy, acc=0.01896792189679219.\n",
    "2020-06-17 06:29:54Z, [6:10] Done fit\n",
    "2020-06-17 06:33:14Z, [6:0] Done fit\n",
    "2020-06-17 06:36:56Z, [6:1] Done fit\n",
    "2020-06-17 06:40:33Z, [6:2] Done fit\n",
    "2020-06-17 06:44:19Z, [6:3] Done fit\n",
    "2020-06-17 06:48:04Z, [6:4] Done fit\n",
    "2020-06-17 06:52:00Z, [6:5] Done fit\n",
    "2020-06-17 06:55:47Z, [6:6] Done fit\n",
    "2020-06-17 06:59:48Z, [6:7] Done fit\n",
    "2020-06-17 07:03:43Z, [6:8] Done fit\n",
    "2020-06-17 07:07:49Z, [6:9] Done fit\n",
    "2020-06-17 07:10:57Z, [6:10] Done fit\n",
    "2020-06-17 07:17:57Z, [6] Done predict_proba\n",
    "2020-06-17 07:17:57Z, [6] Done big_logloss, loss=0.0.\n",
    "2020-06-17 07:17:57Z, [6] Done accuracy, acc=0.01785216178521618.\n",
    "2020-06-17 07:17:57Z, [7:10] Done fit\n",
    "2020-06-17 07:22:02Z, [7:0] Done fit\n",
    "2020-06-17 07:26:15Z, [7:1] Done fit\n",
    "2020-06-17 07:30:39Z, [7:2] Done fit\n",
    "2020-06-17 07:34:58Z, [7:3] Done fit\n",
    "2020-06-17 07:39:25Z, [7:4] Done fit\n",
    "2020-06-17 07:43:49Z, [7:5] Done fit\n",
    "2020-06-17 07:48:24Z, [7:6] Done fit\n",
    "2020-06-17 07:52:55Z, [7:7] Done fit\n",
    "2020-06-17 07:57:46Z, [7:8] Done fit\n",
    "2020-06-17 08:02:24Z, [7:9] Done fit\n",
    "2020-06-17 08:06:09Z, [7:10] Done fit\n",
    "2020-06-17 08:14:36Z, [7] Done predict_proba\n",
    "2020-06-17 08:14:36Z, [7] Done big_logloss, loss=0.0.\n",
    "2020-06-17 08:14:36Z, [7] Done accuracy, acc=0.0200836820083682.\n",
    "2020-06-17 08:14:36Z, [8:10] Done fit\n",
    "2020-06-17 08:19:16Z, [8:0] Done fit\n",
    "2020-06-17 08:24:16Z, [8:1] Done fit\n",
    "2020-06-17 08:29:08Z, [8:2] Done fit\n",
    "2020-06-17 08:34:17Z, [8:3] Done fit\n",
    "2020-06-17 08:39:15Z, [8:4] Done fit\n",
    "2020-06-17 08:44:27Z, [8:5] Done fit\n",
    "2020-06-17 08:49:34Z, [8:6] Done fit\n",
    "2020-06-17 08:55:00Z, [8:7] Done fit\n",
    "2020-06-17 09:00:16Z, [8:8] Done fit\n",
    "2020-06-17 09:05:48Z, [8:9] Done fit\n",
    "2020-06-17 09:09:58Z, [8:10] Done fit\n",
    "2020-06-17 09:19:24Z, [8] Done predict_proba\n",
    "2020-06-17 09:19:24Z, [8] Done big_logloss, loss=0.0.\n",
    "2020-06-17 09:19:24Z, [8] Done accuracy, acc=0.018688981868898186.\n",
    "2020-06-17 09:19:24Z, [9:10] Done fit\n",
    "2020-06-17 09:24:53Z, [9:0] Done fit\n",
    "2020-06-17 09:30:24Z, [9:1] Done fit\n",
    "2020-06-17 09:36:14Z, [9:2] Done fit\n",
    "2020-06-17 09:41:58Z, [9:3] Done fit\n",
    "2020-06-17 09:47:58Z, [9:4] Done fit\n",
    "2020-06-17 09:53:45Z, [9:5] Done fit\n",
    "2020-06-17 09:59:56Z, [9:6] Done fit\n",
    "2020-06-17 10:05:56Z, [9:7] Done fit\n",
    "2020-06-17 10:12:09Z, [9:8] Done fit\n",
    "2020-06-17 10:18:20Z, [9:9] Done fit\n",
    "2020-06-17 10:23:23Z, [9:10] Done fit\n",
    "2020-06-17 10:34:41Z, [9] Done predict_proba\n",
    "2020-06-17 10:34:41Z, [9] Done big_logloss, loss=0.0.\n",
    "2020-06-17 10:34:41Z, [9] Done accuracy, acc=0.02203626220362622.\n",
    "2020-06-17 10:34:41Z, [10:10] Done fit\n",
    "2020-06-17 10:40:48Z, [10:0] Done fit\n",
    "2020-06-17 10:47:31Z, [10:1] Done fit\n",
    "2020-06-17 10:53:59Z, [10:2] Done fit\n",
    "2020-06-17 11:00:42Z, [10:3] Done fit\n",
    "2020-06-17 11:07:12Z, [10:4] Done fit\n",
    "2020-06-17 11:14:02Z, [10:5] Done fit\n",
    "2020-06-17 11:20:43Z, [10:6] Done fit\n",
    "2020-06-17 11:27:41Z, [10:7] Done fit\n",
    "2020-06-17 11:34:28Z, [10:8] Done fit\n",
    "2020-06-17 11:41:37Z, [10:9] Done fit\n",
    "2020-06-17 11:46:58Z, [10:10] Done fit\n",
    "2020-06-17 11:59:06Z, [10] Done predict_proba\n",
    "2020-06-17 11:59:06Z, [10] Done big_logloss, loss=0.0.\n",
    "2020-06-17 11:59:06Z, [10] Done accuracy, acc=0.020362622036262202.\n",
    "2020-06-17 11:59:06Z, [11:10] Done fit\n",
    "2020-06-17 12:06:10Z, [11:0] Done fit\n",
    "2020-06-17 12:13:15Z, [11:1] Done fit\n",
    "2020-06-17 12:20:48Z, [11:2] Done fit\n",
    "2020-06-17 12:28:00Z, [11:3] Done fit\n",
    "2020-06-17 12:35:38Z, [11:4] Done fit\n",
    "2020-06-17 12:43:03Z, [11:5] Done fit\n",
    "2020-06-17 12:50:55Z, [11:6] Done fit\n",
    "2020-06-17 12:58:28Z, [11:7] Done fit\n",
    "2020-06-17 13:06:20Z, [11:8] Done fit\n",
    "2020-06-17 13:13:58Z, [11:9] Done fit\n",
    "2020-06-17 13:20:14Z, [11:10] Done fit\n",
    "2020-06-17 13:34:20Z, [11] Done predict_proba\n",
    "2020-06-17 13:34:20Z, [11] Done big_logloss, loss=0.0.\n",
    "2020-06-17 13:34:20Z, [11] Done accuracy, acc=0.01896792189679219.\n",
    "2020-06-17 13:34:20Z, [12:10] Done fit\n",
    "2020-06-17 13:41:56Z, [12:0] Done fit\n",
    "2020-06-17 13:50:16Z, [12:1] Done fit\n",
    "2020-06-17 13:58:13Z, [12:2] Done fit\n",
    "2020-06-17 14:06:39Z, [12:3] Done fit\n",
    "2020-06-17 14:14:49Z, [12:4] Done fit\n",
    "2020-06-17 14:23:25Z, [12:5] Done fit\n",
    "2020-06-17 14:31:39Z, [12:6] Done fit\n",
    "2020-06-17 14:40:23Z, [12:7] Done fit\n",
    "2020-06-17 14:48:54Z, [12:8] Done fit\n",
    "2020-06-17 14:57:47Z, [12:9] Done fit\n",
    "2020-06-17 15:04:23Z, [12:10] Done fit\n",
    "2020-06-17 15:19:31Z, [12] Done predict_proba\n",
    "2020-06-17 15:19:31Z, [12] Done big_logloss, loss=0.0.\n",
    "2020-06-17 15:19:31Z, [12] Done accuracy, acc=0.021199442119944213.\n",
    "2020-06-17 15:19:31Z, [13:10] Done fit\n",
    "2020-06-17 15:28:18Z, [13:0] Done fit\n",
    "2020-06-17 15:37:06Z, [13:1] Done fit\n",
    "2020-06-17 15:46:23Z, [13:2] Done fit\n",
    "2020-06-17 15:55:24Z, [13:3] Done fit\n",
    "2020-06-17 16:04:49Z, [13:4] Done fit\n",
    "2020-06-17 16:13:59Z, [13:5] Done fit\n",
    "2020-06-17 16:23:34Z, [13:6] Done fit\n",
    "2020-06-17 16:32:50Z, [13:7] Done fit\n",
    "2020-06-17 16:42:36Z, [13:8] Done fit\n",
    "2020-06-17 16:52:04Z, [13:9] Done fit\n",
    "2020-06-17 17:00:04Z, [13:10] Done fit\n",
    "2020-06-17 17:17:24Z, [13] Done predict_proba\n",
    "2020-06-17 17:17:24Z, [13] Done big_logloss, loss=0.0.\n",
    "2020-06-17 17:17:24Z, [13] Done accuracy, acc=0.022594142259414227.\n",
    "2020-06-17 17:17:24Z, [14:10] Done fit\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020-06-18\n",
    "\n",
    "#### that weird logloss ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fu.big_logloss(y_test_enc, y_prob_vec, labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3585,), (3585, 54), (54,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc.shape, y_prob_vec.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34, 45, 18, 41,  5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_enc[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23  7 17  7 20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.43320407e-02, 1.45975398e-02, 1.37405249e-03, 3.63272801e-02,\n",
       "        1.57198533e-02, 1.98668288e-03, 7.11898599e-03, 7.78052211e-02,\n",
       "        9.97787993e-03, 1.23451920e-02, 2.27672025e-03, 5.64281363e-03,\n",
       "        8.06225557e-03, 3.40091810e-03, 6.95233792e-03, 1.42584732e-02,\n",
       "        5.61934011e-03, 9.11897793e-03, 1.73261724e-02, 2.23158412e-02,\n",
       "        1.78813301e-02, 1.10056736e-01, 3.38553800e-03, 1.13068961e-01,\n",
       "        1.50842301e-04, 4.20521423e-02, 7.04979850e-03, 1.20025994e-02,\n",
       "        6.75320439e-03, 7.60487281e-04, 6.79432228e-03, 2.07565799e-02,\n",
       "        7.43526891e-02, 6.23069294e-02, 4.86901738e-02, 3.42086470e-03,\n",
       "        2.00877711e-02, 8.52537458e-04, 3.61542945e-04, 1.03140045e-02,\n",
       "        2.09099017e-02, 2.03171559e-02, 2.66083190e-03, 1.87704358e-02,\n",
       "        1.21015692e-02, 1.26861909e-03, 5.92931965e-03, 1.36819975e-02,\n",
       "        9.47918184e-03, 1.92549676e-02, 7.43823545e-03, 2.45635223e-04,\n",
       "        2.45275471e-04, 3.92340007e-05],\n",
       "       [1.13724703e-02, 4.33668168e-03, 4.03582817e-04, 6.92225341e-03,\n",
       "        1.39332777e-02, 1.61190820e-03, 2.09096842e-03, 2.83393562e-01,\n",
       "        1.66558996e-02, 1.21449009e-02, 2.26105284e-03, 2.58109532e-03,\n",
       "        5.44959726e-03, 2.11735209e-03, 5.09563554e-03, 1.24313263e-02,\n",
       "        7.47357821e-03, 8.27851053e-03, 1.20807439e-02, 1.23237036e-02,\n",
       "        2.18900815e-02, 3.52324881e-02, 4.85089328e-03, 6.39108196e-02,\n",
       "        1.44102727e-03, 1.57984195e-03, 1.51892868e-03, 1.86148211e-02,\n",
       "        1.07575553e-02, 6.51678303e-04, 2.20589489e-02, 2.00667128e-01,\n",
       "        2.56775413e-02, 3.14233303e-02, 3.49258841e-03, 3.00011109e-03,\n",
       "        5.90012316e-03, 5.39436005e-04, 3.09568888e-04, 3.02939955e-03,\n",
       "        8.89371242e-03, 1.55405439e-02, 2.60572648e-03, 1.30862631e-02,\n",
       "        2.64313109e-02, 1.07215543e-03, 4.39102668e-03, 1.69403516e-02,\n",
       "        6.51075132e-03, 3.04438686e-07, 2.46124677e-02, 2.57302687e-04,\n",
       "        1.33520109e-04, 2.01591065e-05],\n",
       "       [1.00219222e-02, 5.82306460e-03, 1.18545606e-04, 9.62727237e-03,\n",
       "        1.74595159e-03, 4.93757986e-03, 1.75451773e-04, 1.60150547e-02,\n",
       "        2.92175030e-03, 1.79107208e-02, 3.83758987e-03, 3.24463705e-04,\n",
       "        1.78701475e-01, 7.56689208e-03, 1.20573631e-02, 1.69108659e-02,\n",
       "        1.32384067e-02, 3.77053291e-01, 2.63878852e-02, 5.67842275e-03,\n",
       "        1.37230046e-02, 1.97000476e-03, 3.55870766e-03, 6.42400561e-03,\n",
       "        4.73674620e-03, 1.61048747e-03, 4.84825636e-04, 1.72724240e-02,\n",
       "        4.69100941e-03, 5.35734929e-03, 1.17457239e-02, 6.79301396e-02,\n",
       "        9.28889867e-03, 3.11903120e-03, 2.03564006e-04, 5.01161558e-04,\n",
       "        3.05468799e-03, 8.99023871e-05, 4.37883806e-04, 7.10516889e-03,\n",
       "        2.13857173e-04, 4.73770825e-03, 9.35826625e-04, 2.34301295e-03,\n",
       "        8.64369981e-03, 1.11553457e-03, 1.11323467e-03, 4.46197856e-03,\n",
       "        3.86142085e-04, 7.99465701e-02, 2.11091097e-02, 2.22372037e-04,\n",
       "        3.93820781e-04, 1.83790180e-05],\n",
       "       [6.15562405e-03, 1.29264016e-02, 3.43316569e-05, 1.84753351e-02,\n",
       "        3.68996477e-03, 3.79020785e-04, 2.67782481e-04, 2.20118806e-01,\n",
       "        6.42478559e-03, 7.09167728e-03, 1.22226833e-04, 2.06250304e-04,\n",
       "        5.14525454e-04, 8.10637954e-04, 4.99898242e-03, 7.97380880e-03,\n",
       "        2.66266055e-02, 5.65080298e-03, 4.39017778e-03, 1.44837312e-02,\n",
       "        3.19633521e-02, 9.35966223e-02, 8.54910258e-03, 8.89217854e-02,\n",
       "        1.52460486e-02, 2.34667794e-03, 1.19347812e-03, 2.14530826e-02,\n",
       "        2.05857004e-03, 4.24271071e-04, 4.88946512e-02, 7.23616686e-03,\n",
       "        6.72372654e-02, 2.21090782e-02, 3.60832480e-03, 3.60246957e-03,\n",
       "        7.59943016e-03, 4.70407685e-04, 2.31332844e-04, 3.01281326e-02,\n",
       "        1.15565932e-03, 4.49882867e-03, 2.93037528e-03, 3.52278426e-02,\n",
       "        3.82017009e-02, 5.27505018e-03, 2.20251386e-03, 4.30632345e-02,\n",
       "        2.49090809e-02, 3.89365330e-02, 4.72581480e-03, 5.77592698e-04,\n",
       "        8.27158656e-05, 1.33898641e-06],\n",
       "       [5.33144362e-02, 8.74674600e-03, 4.70552547e-03, 1.82881486e-02,\n",
       "        2.31490545e-02, 3.01193306e-03, 1.30525546e-03, 2.50697993e-02,\n",
       "        1.84454303e-02, 1.49767136e-03, 1.64568797e-02, 2.07309797e-03,\n",
       "        1.06080659e-02, 1.49900885e-03, 5.82267111e-03, 6.07391819e-02,\n",
       "        4.25334387e-02, 5.67580089e-02, 5.10841459e-02, 3.16724591e-02,\n",
       "        8.36721733e-02, 4.00946848e-02, 1.14471940e-02, 1.28054004e-02,\n",
       "        6.31947676e-03, 4.25715744e-03, 1.77965776e-04, 1.90951396e-02,\n",
       "        1.94816831e-02, 1.11032082e-02, 4.49019149e-02, 5.47672249e-02,\n",
       "        1.67210102e-02, 1.22997165e-03, 2.03423835e-02, 8.61785095e-03,\n",
       "        1.15226686e-03, 6.17770804e-03, 3.93093890e-03, 5.98935690e-03,\n",
       "        1.11780781e-02, 8.42456613e-03, 1.58727134e-03, 2.75659505e-02,\n",
       "        4.13099751e-02, 1.52806542e-03, 5.96439606e-03, 4.94650193e-02,\n",
       "        1.48439817e-02, 2.23171376e-02, 6.10806374e-03, 3.60912643e-04,\n",
       "        2.03361284e-04, 7.75708177e-05]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahh ... so if all the values are super low, logloss fades?\n",
    "print(np.argmax(y_prob_vec[:5, :], axis=1))\n",
    "y_prob_vec[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0986122886681098\n",
      "2.1094237467877998e-15\n",
      "23.025850929940457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# log_loss(y[:1], y_prob[:1], labels=labels); labe\n",
    "a, b = np.array([0, 0, 0]), np.array([[0, 0., 0],\n",
    "                                      [0., 0, 0],\n",
    "                                      [0., 0, 0]])\n",
    "print(log_loss(a, b, labels=['a', 'b', 'c']))\n",
    "\n",
    "a, b = np.array(['a', 'a', 'a']), np.array([[0, 0., 0],\n",
    "                                      [0., 0, 0],\n",
    "                                      [0., 0, 0]])\n",
    "print(log_loss(a, b, labels=['a', 'b', 'c']))\n",
    "\n",
    "a, b = np.array(['a', 'a', 'a']), np.array([[1, 0., 0],\n",
    "                                      [1., 0, 0],\n",
    "                                      [1., 0, 0]])\n",
    "print(log_loss(a, b, labels=['a', 'b', 'c']))\n",
    "\n",
    "a, b = np.array(['a', 'a', 'a']), np.array([[0, 1., 0],\n",
    "                                      [0., 1, 0],\n",
    "                                      [1., 0, 0]])\n",
    "print(log_loss(a, b, labels=['a', 'b', 'c']))\n",
    "\n",
    "# Ok.. so whoops.. log loss was 0.0 because my y_true actually didnt correspond w/ the labels!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.921754919063358"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fu.big_logloss(y_test_enc, y_prob_vec, labels=list(range(54)))    \n",
    "# makes sense now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3585,) (3585, 54) (54,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test_enc.shape, y_prob_vec.shape, labels.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
