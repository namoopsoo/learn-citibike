### Citibike Project: Can your Destination be Predicted

#### Motivation
I think sometimes the most interesting projects live behind ideas that sound impractical or even crazy. That's why I thought
it would be fun to use the Citibike bike share trip data to try and predict a person's destination based on what we know.

_Roughly speaking trip data looks like this_

```
"tripduration","starttime","stoptime","start station id","start station name","start station latitude","start station longitude","end station id","end station name","end station latitude","end station longitude","bikeid","usertype","birth year","gender"
"171","10/1/2015 00:00:02","10/1/2015 00:02:54","388","W 26 St & 10 Ave","40.749717753","-74.002950346","494","W 26 St & 8 Ave","40.74734825","-73.99723551","24302","Subscriber","1973","1"
"593","10/1/2015 00:00:02","10/1/2015 00:09:55","518","E 39 St & 2 Ave","40.74780373","-73.9734419","438","St Marks Pl & 1 Ave","40.72779126","-73.98564945","19904","Subscriber","1990","1"
"233","10/1/2015 00:00:11","10/1/2015 00:04:05","447","8 Ave & W 52 St","40.76370739","-73.9851615","447","8 Ave & W 52 St","40.76370739","-73.9851615","17797","Subscriber","1984","1"
"250","10/1/2015 00:00:15","10/1/2015 00:04:25","336","Sullivan St & Washington Sq","40.73047747","-73.99906065","223","W 13 St & 7 Ave","40.73781509","-73.99994661","23966","Subscriber","1984","1"
"528","10/1/2015 00:00:17","10/1/2015 00:09:05","3107","Bedford Ave & Nassau Ave","40.72311651","-73.95212324","539","Metropolitan Ave & Bedford Ave","40.71534825","-73.96024116","16246","Customer","","0"
"440","10/1/2015 00:00:17","10/1/2015 00:07:37","3107","Bedford Ave & Nassau Ave","40.72311651","-73.95212324","539","Metropolitan Ave & Bedford Ave","40.71534825","-73.96024116","23698","Customer","","0"
```

The data if fairly clean and regular, so I thought this was a fun data set to sharpen my teeth on.

#### Quick Bird's Eye  of my Journey
* First started just [looking](#more-on-this-data) at this data.
* Just out of curiosity, as a first mini starter project I decided to look at the [relationship between rider age and speed](#speed-and-age)
* I realized pretty early that the bike station target was too small, so I started using the Google Geolocation API to get broader location data such
as _zip codes_ and _neighborhoods_ . [geolocation](#need-additional-location-data)
* I also thought on a high level that knowing whether you got on your bike at `4:05` in the afternoon versus `4:06` shouldn't 
influence my learning algorithm, so I added some more [transformations](#time-bucketing).
* I compared the prediction accuracy of the new geolocation data as a [first stab](#comparing-geolocation-granularities)
* I ran through a couple of modeling scenarios [here](#deeper-into-the-weeds)


#### More on this data
* When I started looking at this data, there were 400+ stations for docking your citibike.
* There is age, and some of the riders were actually born in the 1800s, which is kind of cool.

```python
df = load_data('data/201509_10-citibike-tripdata.csv.annotated.100000.06112016T1814.csv')

In [6]: df['birth year'].describe()
Out[6]: 
count    83171.000000
mean      1977.149680
std         11.400096
min       1885.000000
25%       1969.000000
50%       1980.000000
75%       1986.000000
max       1999.000000
Name: birth year, dtype: float64
```

#### Speed and Age
Turns out that you need to know the miles per the longitude degree at a particular latitude on our planet. So for our particular location, 
at lat around `40.723` and using the earth's radius of about `3958 miles` , we have about `52.3 miles/longitude degree`
here in NYC. 
 
So from there, looking at some of the speed data just involved looking at the tripdata trip time and calculating the 
cartesian distance. 
<img src="https://github.com/namoopsoo/learn-citibike/blob/master/notes/assets/Screen%20Shot%202019-05-21%20at%2011.02.41%20AM.png"
width="435" height="307"  >
1758 × 1238

(More on the code [here](https://github.com/namoopsoo/learn-citibike/blob/master/bikelearn/utils.py#L86) )
(Also more detail on this analysis in the main [jupyter notebook](https://github.com/namoopsoo/learn-citibike/blob/master/project%20report.ipynb)) 

#### Need additional location data
* With the 400+ stations, trying to predict a multi-class problem of this sort with basic machine learning algorithms
would not be a way to get quick results to help keep the project going, so I decided to constrain the problem. I ended up
turning to the Google Geocoding API. Using this data because a side project in its own right, because parsing through 
the Google geolocation data can get pretty hairy! 

_The meat of the output can look like this, for the docking station "1st Avenue & E 15th St"_
```python
{
'raw_result': [{u'address_components': [{u'long_name': u'1st Avenue',
                u'short_name': u'1st Avenue',
                u'types': [u'route']},
               {u'long_name': u'Midtown',
                u'short_name': u'Midtown',
                u'types': [u'neighborhood',
                           u'political']},
               {u'long_name': u'Manhattan',
                u'short_name': u'Manhattan',
                u'types': [u'sublocality_level_1',
                           u'sublocality',
                           u'political']},
               {u'long_name': u'New York',
                u'short_name': u'New York',
                u'types': [u'locality',
                           u'political']},
               {u'long_name': u'New York County',
                u'short_name': u'New York County',
                u'types': [u'administrative_area_level_2',
                           u'political']},
               {u'long_name': u'New York',
                u'short_name': u'NY',
                u'types': [u'administrative_area_level_1',
                           u'political']},
               {u'long_name': u'United States',
                u'short_name': u'US',
                u'types': [u'country',
                           u'political']},
               {u'long_name': u'10003',
                u'short_name': u'10003',
                u'types': [u'postal_code']}],
               u'formatted_address': u'1st Avenue & E 15th St, New York, NY 10003, USA',
               }
```


* Some of the challenges here were that the outputs from the API were not always consistent. The above output shows that the  
`'neighborhood'` is `'Midtown'`, but because there were 400+ stations, I did not initially notice that sometimes the `neighborhood`
was missing or that the `zip code` was missing. That ended up throwing off my code a couple of times. 
* It turned out that the _street intersection_ was not an ideal clean data input to the API. For instance `E 3 St & 1 Ave, NY` was understood by the API as just a street or a `route` as it is called, ( [raw output](https://github.com/namoopsoo/learn-citibike/blob/master/notes/assets/E%203%20St%20&%201%20Ave,%20NY.md) ). Later on I ended up refactoring this to 
use the raw _latitude and longitude_ .
* However, I eventually noticed that often times the _docking stations_ were on the edge of neighborhoods. So I literally had 
edge cases! The Neighborhood would come back blank and I ended up having to fill in a lot of that data by hand anyhow!

<img src="https://github.com/namoopsoo/learn-citibike/blob/master/notes/assets/Screen%20Shot%202018-12-04%20at%2012.11.42%20PM.png"
width="457" height="328">
1830 × 1314

* Also the data calls were not free and I ended up building a small _caching layer_ with _redis_ . 
* The other reason I had done that was that I would often work out of cafes where the Wifi was spotty and I didn't want an internet
connection to hold me back.
*  I think in hind-sight, I could have avoided some of the automation here and just decoupled the data fetch so that I wouldn't have to
worry about that internet connection. 
* Of course every time I wanted to add additional data from Citibike, there would be new docking stations and I had to get back to making sure
my station location data was correct, so that bad data did not impact predictions.

#### Time bucketing 
In order to get better information from the source time, the source time was bucketted into 24 hour-buckets per day. That is since a ride starting at 1:04:23pm shouldn't be treated as being too different from a ride departing at 1:05:24pm . There is more value in intuitively clustering the rides.

#### Comparing Geolocation Granularities
There are about 463 stations found in the dataset, 28 neighborhoods, representing 49 postal codes and 3 out of 5 boroughs,

So using  the `(start time bucket, start station id, age, gender)` as the inputs and  with `RandomizedLogisticRegression` as a classifier ,
for about a months worth of trip data, I saw roughly the following comparison.

```python
{'end station id': OrderedDict([('training',
               OrderedDict([('accuracy_score', 0.041432771986099973),
                            ('f1_score', 0.015138704086611844),
                            ('recall_score', 0.041432771986099973),
                            ('precision_score', 0.016942125433308568)])),
              ('holdout',
               OrderedDict([('accuracy_score', 0.031533939070016032),
                            ('f1_score', 0.0093952628045424723),
                            ('recall_score', 0.031533939070016032),
                            ('precision_score', 0.0067157290264759353)]))]),
 'end_neighborhood': OrderedDict([('training',
               OrderedDict([('accuracy_score', 0.39047231270358307),
                            ('f1_score', 0.28885663229134789),
                            ('recall_score', 0.39047231270358307),
                            ('precision_score', 0.26445041603375502)])),
              ('holdout',
               OrderedDict([('accuracy_score', 0.39630836047774159),
                            ('f1_score', 0.2935527390151364),
                            ('recall_score', 0.39630836047774159),
                            ('precision_score', 0.26579390443173939)]))]),
 'end_postal_code': OrderedDict([('training',
               OrderedDict([('accuracy_score', 0.14129127122042506),
                            ('f1_score', 0.068340173428106887),
                            ('recall_score', 0.14129127122042506),
                            ('precision_score', 0.068168259430770747)])),
              ('holdout',
               OrderedDict([('accuracy_score', 0.13361838588989844),
                            ('f1_score', 0.064738931917963718),
                            ('recall_score', 0.13361838588989844),
                            ('precision_score', 0.067139580345228156)]))]),
 'end_sublocality': OrderedDict([('training',
               OrderedDict([('accuracy_score', 0.95354786589470852),
                            ('f1_score', 0.95209028150037733),
                            ('recall_score', 0.95354786589470852),
                            ('precision_score', 0.95217920885515972)])),
              ('holdout',
               OrderedDict([('accuracy_score', 0.9493807215939688),
                            ('f1_score', 0.94990282092170586),
                            ('recall_score', 0.9493807215939688),
                            ('precision_score', 0.95132373575480056)]))])}
```

* The rough accuracies for prediction, end station id (~3%), postal code (~13%), neighborhood (~40%), and borough (~95%), using the small dataset, shows the rough differences in what happens when you reduce the number of possible outputs.
* Overall this gave me the motivation to focus on the `neighborhood` as a target to try to improve upon.
* More in the [jupyter notebook](https://render.githubusercontent.com/view/ipynb?commit=b2beb2af23f4f803a059161aeeb1a8e628a1bd4b&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e616d6f6f70736f6f2f6c6561726e2d6369746962696b652f623262656232616632336634663830336130353931363161656562316138653632386131626434622f70726f6a6563742532307265706f72742e6970796e62&nwo=namoopsoo%2Flearn-citibike&path=project+report.ipynb&repository_id=60489657&repository_type=Repository#A-basic-learning-strategy-is-used)

#### Deeper into the weeds
I compared the `SGDClassifier` with the `LogisticRegression` classifier (which I believe just uses Gradient Descent, while the `SGDClassifier`  classifier is also a Logistic Regression classifier, but it uses Stochastic Gradient Descent).

I also tried applying Standard Scaling to my input data after learning that  `scikit learn` 's `LogisticRegression` implementation is sensitive unless the input data has a `mean=0` and `variance=1` .  Indeed per the below this helped a little.

<img src="https://github.com/namoopsoo/learn-citibike/blob/master/notes/assets/Screen%20Shot%202019-05-21%20at%2012.12.54%20PM.png"
width="637" height="302">


More in the jupyter [notebook](https://render.githubusercontent.com/view/ipynb?commit=b2beb2af23f4f803a059161aeeb1a8e628a1bd4b&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6e616d6f6f70736f6f2f6c6561726e2d6369746962696b652f623262656232616632336634663830336130353931363161656562316138653632386131626434622f70726f6a6563742532307265706f72742e6970796e62&nwo=namoopsoo%2Flearn-citibike&path=project+report.ipynb&repository_id=60489657&repository_type=Repository#Also-comparing-with-additional-classifiers)




