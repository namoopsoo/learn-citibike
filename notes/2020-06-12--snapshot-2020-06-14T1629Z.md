

```python
import pandas as pd
import xgboost as xgb
from xgboost import XGBClassifier
import datetime; import pytz
import matplotlib as plt
from scipy.special import softmax
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split # (*arrays, **options)
import numpy as np
from sklearn.metrics import log_loss
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

from joblib import dump, load
import joblib
import os
from sklearn.metrics import confusion_matrix, mean_squared_error
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
```


```python
datadir = '/opt/data'
localdir = '/opt/program'
tripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv')
stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',
                        index_col=0)
```


```python
os.getcwd()
```




    '/opt/program'




```python
# load model from 2020-06-10 notebook
bundle = joblib.load('/opt/program/artifacts/2020-06-11T041641Z/bundle.joblib')
```


```python
bundle
```




    {'notebook': '2020-06-10-again',
     'model': XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                   colsample_bynode=1, colsample_bytree=1, gamma=0,
                   learning_rate=0.1, max_delta_step=0, max_depth=3,
                   min_child_weight=1, missing=nan, n_estimators=100, n_jobs=1,
                   nthread=None, objective='multi:softprob', random_state=0,
                   reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
                   silent=None, subsample=1, verbosity=1),
     'actuals': array([60, 60, 60, ...,  2,  2,  2]),
     'predictions': array([46, 46, 46, ...,  2,  2,  2]),
     'confusion_matrix': array([[ 193,   64,   14, ...,    0,    0,  150],
            [  27, 1167,   16, ...,    0,    0,   28],
            [   4,    6,  379, ...,    0,    0,  236],
            ...,
            [   0,    0,   17, ...,    0,    0,   46],
            [  67,  599,   24, ...,    0,    0,   78],
            [  19,    9,  196, ...,    0,    0, 2017]]),
     'walltime_train': '19min 45s',
     'preproc': {'le': LabelEncoder(),
      'one_hot_enc': OneHotEncoder(categories=[['Alphabet City', 'Battery Park City',
                                 'Bedford-Stuyvesant', 'Bloomingdale', 'Boerum Hill',
                                 'Bowery', 'Broadway Triangle', 'Brooklyn Heights',
                                 'Brooklyn Navy Yard', 'Carnegie Hill',
                                 'Carroll Gardens', 'Central Park', 'Chelsea',
                                 'Chinatown', 'Civic Center', 'Clinton Hill',
                                 'Cobble Hill', 'Columbia Street Waterfront District',
                                 'Downtown Brooklyn', 'Dumbo', 'East Harlem',
                                 'East Village', 'East Williamsburg',
                                 'Financial District', 'Flatiron District',
                                 'Fort Greene', 'Fulton Ferry District',
                                 'Garment District', 'Governors Island', 'Gowanus', ...],
                                [0, 1, 2]],
                    drop=None, dtype=<class 'numpy.float64'>, handle_unknown='error',
                    sparse=True)}}




```python
# quick manual accuracy computation
bundle['actuals'].shape, bundle['predictions'].shape
correct = len([i for i, _ in enumerate(bundle['actuals'])
              if bundle['actuals'][i] == bundle['predictions'][i]
              ])
print({'overall accuracy': correct/len(bundle['actuals']), 
       'num_correct': correct, 'total': len(bundle['actuals'])})
print(bundle['confusion_matrix'].shape)

confusion_diagonal = [bundle['confusion_matrix'][i][j]
                                  for i in range(54)
                                  for j in range(54)
                                  if i == j]
print(confusion_diagonal)
print('confusion, sum, diagonal', sum(confusion_diagonal))
```

    {'overall accuracy': 0.1541161182619253, 'num_correct': 64992, 'total': 421708}
    (54, 54)
    [193, 1167, 379, 0, 0, 1406, 0, 1039, 30505, 0, 685, 557, 67, 62, 49, 0, 4534, 0, 2583, 109, 0, 0, 2766, 2003, 0, 0, 0, 236, 0, 2722, 0, 0, 612, 6064, 581, 0, 0, 0, 0, 24, 0, 0, 0, 0, 0, 0, 4632, 0, 0, 0, 0, 0, 0, 2017]
    confusion, sum, diagonal 64992



```python
model = bundle['model']
print(len(model.feature_importances_))
print(sorted(model.feature_importances_))
```

    78
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00057195826, 0.00096085493, 0.0019978876, 0.0028608162, 0.003014325, 0.0031509278, 0.005542396, 0.0055443724, 0.0056226645, 0.0064963517, 0.0066661313, 0.007307959, 0.007578683, 0.0077737863, 0.008284873, 0.00866166, 0.009315344, 0.010423137, 0.010517005, 0.010585468, 0.010727769, 0.012014797, 0.012143584, 0.012632706, 0.01298739, 0.013684955, 0.014033192, 0.014308239, 0.014311185, 0.016608043, 0.017402876, 0.017734889, 0.017809916, 0.017948298, 0.018543119, 0.01898507, 0.019225849, 0.02069202, 0.020770036, 0.021067692, 0.02152036, 0.022673236, 0.024081945, 0.02418218, 0.02421692, 0.026871085, 0.027934289, 0.02844202, 0.030187974, 0.032351416, 0.033994444, 0.035514593, 0.03653184, 0.036576174, 0.044999287, 0.04772858, 0.05568539]



```python
# 78 features
model.classes_
```




    array([ 0,  1,  2,  4,  5,  7,  8, 11, 12, 13, 14, 15, 17, 18, 19, 21, 23,
           24, 25, 26, 27, 30, 32, 33, 34, 36, 37, 39, 40, 42, 43, 44, 45, 46,
           47, 48, 49, 50, 51, 52, 53, 57, 58, 60, 62, 63, 64, 65, 66, 67, 68,
           71, 72, 73])




```python
import fresh.utils as fu
from importlib import reload
reload(fu)
```




    <module 'fresh.utils' from '/opt/program/fresh/utils.py'>




```python
X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)
# = sorted(stationsdf.neighborhood.unique().tolist())


```


```python
bundle['preproc']['one_hot_enc'].categories
```




    [['Alphabet City',
      'Battery Park City',
      'Bedford-Stuyvesant',
      'Bloomingdale',
      'Boerum Hill',
      'Bowery',
      'Broadway Triangle',
      'Brooklyn Heights',
      'Brooklyn Navy Yard',
      'Carnegie Hill',
      'Carroll Gardens',
      'Central Park',
      'Chelsea',
      'Chinatown',
      'Civic Center',
      'Clinton Hill',
      'Cobble Hill',
      'Columbia Street Waterfront District',
      'Downtown Brooklyn',
      'Dumbo',
      'East Harlem',
      'East Village',
      'East Williamsburg',
      'Financial District',
      'Flatiron District',
      'Fort Greene',
      'Fulton Ferry District',
      'Garment District',
      'Governors Island',
      'Gowanus',
      'Gramercy Park',
      'Greenpoint',
      'Greenwich Village',
      "Hell's Kitchen",
      'Hudson Square',
      'Hunters Point',
      'Kips Bay',
      'Korea Town',
      'Lenox Hill',
      'Lincoln Square',
      'Little Italy',
      'Long Island City',
      'Lower East Side',
      'Lower Manhattan',
      'Meatpacking District',
      'Midtown',
      'Midtown East',
      'Midtown West',
      'Murray Hill',
      'NoHo',
      'NoMad',
      'Nolita',
      'Park Slope',
      'Peter Cooper Village',
      'Prospect Heights',
      'Prospect Park',
      'Red Hook',
      'Rose Hill',
      'SoHo',
      'Stuyvesant Heights',
      'Stuyvesant Town',
      'Sunset Park',
      'Sutton Place',
      'Theater District',
      'Tribeca',
      'Tudor City',
      'Two Bridges',
      'Ukrainian Village',
      'Union Square',
      'Upper East Side',
      'Upper West Side',
      'Vinegar Hill',
      'West Village',
      'Williamsburg',
      'Yorkville'],
     [0, 1, 2]]




```python
len(bundle['preproc']['one_hot_enc'].categories[0]), len(bundle['preproc']['one_hot_enc'].categories[1])
```




    (75, 3)




```python
features = ([f'feature_start_neighborhood={x}' for x in bundle['preproc']['one_hot_enc'].categories[0]]
           + [f'feature_gender={x}' for x in bundle['preproc']['one_hot_enc'].categories[1]])
print(features)
```

    ['feature_start_neighborhood=Alphabet City', 'feature_start_neighborhood=Battery Park City', 'feature_start_neighborhood=Bedford-Stuyvesant', 'feature_start_neighborhood=Bloomingdale', 'feature_start_neighborhood=Boerum Hill', 'feature_start_neighborhood=Bowery', 'feature_start_neighborhood=Broadway Triangle', 'feature_start_neighborhood=Brooklyn Heights', 'feature_start_neighborhood=Brooklyn Navy Yard', 'feature_start_neighborhood=Carnegie Hill', 'feature_start_neighborhood=Carroll Gardens', 'feature_start_neighborhood=Central Park', 'feature_start_neighborhood=Chelsea', 'feature_start_neighborhood=Chinatown', 'feature_start_neighborhood=Civic Center', 'feature_start_neighborhood=Clinton Hill', 'feature_start_neighborhood=Cobble Hill', 'feature_start_neighborhood=Columbia Street Waterfront District', 'feature_start_neighborhood=Downtown Brooklyn', 'feature_start_neighborhood=Dumbo', 'feature_start_neighborhood=East Harlem', 'feature_start_neighborhood=East Village', 'feature_start_neighborhood=East Williamsburg', 'feature_start_neighborhood=Financial District', 'feature_start_neighborhood=Flatiron District', 'feature_start_neighborhood=Fort Greene', 'feature_start_neighborhood=Fulton Ferry District', 'feature_start_neighborhood=Garment District', 'feature_start_neighborhood=Governors Island', 'feature_start_neighborhood=Gowanus', 'feature_start_neighborhood=Gramercy Park', 'feature_start_neighborhood=Greenpoint', 'feature_start_neighborhood=Greenwich Village', "feature_start_neighborhood=Hell's Kitchen", 'feature_start_neighborhood=Hudson Square', 'feature_start_neighborhood=Hunters Point', 'feature_start_neighborhood=Kips Bay', 'feature_start_neighborhood=Korea Town', 'feature_start_neighborhood=Lenox Hill', 'feature_start_neighborhood=Lincoln Square', 'feature_start_neighborhood=Little Italy', 'feature_start_neighborhood=Long Island City', 'feature_start_neighborhood=Lower East Side', 'feature_start_neighborhood=Lower Manhattan', 'feature_start_neighborhood=Meatpacking District', 'feature_start_neighborhood=Midtown', 'feature_start_neighborhood=Midtown East', 'feature_start_neighborhood=Midtown West', 'feature_start_neighborhood=Murray Hill', 'feature_start_neighborhood=NoHo', 'feature_start_neighborhood=NoMad', 'feature_start_neighborhood=Nolita', 'feature_start_neighborhood=Park Slope', 'feature_start_neighborhood=Peter Cooper Village', 'feature_start_neighborhood=Prospect Heights', 'feature_start_neighborhood=Prospect Park', 'feature_start_neighborhood=Red Hook', 'feature_start_neighborhood=Rose Hill', 'feature_start_neighborhood=SoHo', 'feature_start_neighborhood=Stuyvesant Heights', 'feature_start_neighborhood=Stuyvesant Town', 'feature_start_neighborhood=Sunset Park', 'feature_start_neighborhood=Sutton Place', 'feature_start_neighborhood=Theater District', 'feature_start_neighborhood=Tribeca', 'feature_start_neighborhood=Tudor City', 'feature_start_neighborhood=Two Bridges', 'feature_start_neighborhood=Ukrainian Village', 'feature_start_neighborhood=Union Square', 'feature_start_neighborhood=Upper East Side', 'feature_start_neighborhood=Upper West Side', 'feature_start_neighborhood=Vinegar Hill', 'feature_start_neighborhood=West Village', 'feature_start_neighborhood=Williamsburg', 'feature_start_neighborhood=Yorkville', 'feature_gender=0', 'feature_gender=1', 'feature_gender=2']



```python
importances = model.feature_importances_
labeled_importances = [[features[i], importances[i]] for i in range(78)]
sorted(labeled_importances, key=lambda x:x[1])
```




    [['feature_start_neighborhood=Bloomingdale', 0.0],
     ['feature_start_neighborhood=Broadway Triangle', 0.0],
     ['feature_start_neighborhood=Carnegie Hill', 0.0],
     ['feature_start_neighborhood=Carroll Gardens', 0.0],
     ['feature_start_neighborhood=Cobble Hill', 0.0],
     ['feature_start_neighborhood=East Harlem', 0.0],
     ['feature_start_neighborhood=East Williamsburg', 0.0],
     ['feature_start_neighborhood=Governors Island', 0.0],
     ['feature_start_neighborhood=Gowanus', 0.0],
     ['feature_start_neighborhood=Greenpoint', 0.0],
     ['feature_start_neighborhood=Hunters Point', 0.0],
     ['feature_start_neighborhood=Lenox Hill', 0.0],
     ['feature_start_neighborhood=Long Island City', 0.0],
     ['feature_start_neighborhood=Prospect Heights', 0.0],
     ['feature_start_neighborhood=Prospect Park', 0.0],
     ['feature_start_neighborhood=Red Hook', 0.0],
     ['feature_start_neighborhood=Stuyvesant Heights', 0.0],
     ['feature_start_neighborhood=Sunset Park', 0.0],
     ['feature_start_neighborhood=Upper East Side', 0.0],
     ['feature_start_neighborhood=Upper West Side', 0.0],
     ['feature_start_neighborhood=Yorkville', 0.0],
     ['feature_gender=2', 0.00057195826],
     ['feature_gender=1', 0.00096085493],
     ['feature_start_neighborhood=Tudor City', 0.0019978876],
     ['feature_gender=0', 0.0028608162],
     ['feature_start_neighborhood=Korea Town', 0.003014325],
     ['feature_start_neighborhood=Sutton Place', 0.0031509278],
     ['feature_start_neighborhood=Rose Hill', 0.005542396],
     ['feature_start_neighborhood=NoHo', 0.0055443724],
     ['feature_start_neighborhood=NoMad', 0.0056226645],
     ['feature_start_neighborhood=Union Square', 0.0064963517],
     ['feature_start_neighborhood=Little Italy', 0.0066661313],
     ['feature_start_neighborhood=Gramercy Park', 0.007307959],
     ['feature_start_neighborhood=Peter Cooper Village', 0.007578683],
     ['feature_start_neighborhood=Flatiron District', 0.0077737863],
     ['feature_start_neighborhood=Nolita', 0.008284873],
     ['feature_start_neighborhood=Lower Manhattan', 0.00866166],
     ['feature_start_neighborhood=SoHo', 0.009315344],
     ['feature_start_neighborhood=Stuyvesant Town', 0.010423137],
     ['feature_start_neighborhood=Garment District', 0.010517005],
     ['feature_start_neighborhood=Meatpacking District', 0.010585468],
     ['feature_start_neighborhood=Murray Hill', 0.010727769],
     ['feature_start_neighborhood=Kips Bay', 0.012014797],
     ['feature_start_neighborhood=Hudson Square', 0.012143584],
     ['feature_start_neighborhood=Ukrainian Village', 0.012632706],
     ['feature_start_neighborhood=Midtown', 0.01298739],
     ['feature_start_neighborhood=East Village', 0.013684955],
     ['feature_start_neighborhood=Park Slope', 0.014033192],
     ['feature_start_neighborhood=Vinegar Hill', 0.014308239],
     ['feature_start_neighborhood=Bowery', 0.014311185],
     ['feature_start_neighborhood=Chinatown', 0.016608043],
     ['feature_start_neighborhood=Alphabet City', 0.017402876],
     ['feature_start_neighborhood=Lincoln Square', 0.017734889],
     ['feature_start_neighborhood=Dumbo', 0.017809916],
     ['feature_start_neighborhood=Brooklyn Navy Yard', 0.017948298],
     ['feature_start_neighborhood=Civic Center', 0.018543119],
     ['feature_start_neighborhood=Theater District', 0.01898507],
     ['feature_start_neighborhood=Fulton Ferry District', 0.019225849],
     ['feature_start_neighborhood=West Village', 0.02069202],
     ['feature_start_neighborhood=Midtown West', 0.020770036],
     ['feature_start_neighborhood=Midtown East', 0.021067692],
     ['feature_start_neighborhood=Boerum Hill', 0.02152036],
     ['feature_start_neighborhood=Two Bridges', 0.022673236],
     ["feature_start_neighborhood=Hell's Kitchen", 0.024081945],
     ['feature_start_neighborhood=Greenwich Village', 0.02418218],
     ['feature_start_neighborhood=Columbia Street Waterfront District',
      0.02421692],
     ['feature_start_neighborhood=Battery Park City', 0.026871085],
     ['feature_start_neighborhood=Chelsea', 0.027934289],
     ['feature_start_neighborhood=Lower East Side', 0.02844202],
     ['feature_start_neighborhood=Downtown Brooklyn', 0.030187974],
     ['feature_start_neighborhood=Tribeca', 0.032351416],
     ['feature_start_neighborhood=Clinton Hill', 0.033994444],
     ['feature_start_neighborhood=Brooklyn Heights', 0.035514593],
     ['feature_start_neighborhood=Bedford-Stuyvesant', 0.03653184],
     ['feature_start_neighborhood=Financial District', 0.036576174],
     ['feature_start_neighborhood=Fort Greene', 0.044999287],
     ['feature_start_neighborhood=Williamsburg', 0.04772858],
     ['feature_start_neighborhood=Central Park', 0.05568539]]




```python
# Perhaps indeed the starting locations which are so un-important, basically have just way too many
# destinations, or maybe there are just not enough trips involving those stations.
# But anyway, I think ultimately the importances which are there 
#    across different cross validation folds are the ones to focus on
```


```python
# retry that multi-logloss ...
rng = np.random.RandomState(31337)
indices1 = []
kf = KFold(n_splits=2, shuffle=True, random_state=rng)
for train_index, test_index in kf.split(X):
    indices1.append([train_index, test_index])

indices2 = []
kf = KFold(n_splits=2, shuffle=True, random_state=rng)
for train_index, test_index in kf.split(X):
    indices2.append([train_index, test_index])
    

```


```python
# hmm doesnt look the same though, guess that random seed doesnt work deterministically?
indices1[0][:5] , indices2[0][:5]

```




    ([array([     0,      2,      3, ..., 843411, 843412, 843415]),
      array([     1,      7,      8, ..., 843410, 843413, 843414])],
     [array([     5,      7,      9, ..., 843412, 843413, 843414]),
      array([     0,      1,      2, ..., 843408, 843411, 843415])])




```python
#log_loss(y_true_enc, y_prob, labels=self.labels)
#help(log_loss)
bundle['preproc']['le'].classes_.shape
```




    (75,)




```python
X_transformed = bundle['preproc']['one_hot_enc'].transform(X[:1000])
y_prob = model.predict_proba(X_transformed)
```


```python
y_prob[0].shape
```




    (54,)




```python
# ok got to clear up this inconsistency.. so, len(bundle['preproc']['one_hot_enc'].categories[0]) => 75
# but there are I believe only 54 neighborhoods actually represented in the data itself,

from collections import Counter
print(len(bundle['preproc']['one_hot_enc'].categories[0]))
print(len(dict(Counter([x[0] for x in X]))))
```

    75
    54



```python
sorted(list(dict(Counter(y)).keys()))
```




    ['Alphabet City',
     'Battery Park City',
     'Bedford-Stuyvesant',
     'Boerum Hill',
     'Bowery',
     'Brooklyn Heights',
     'Brooklyn Navy Yard',
     'Central Park',
     'Chelsea',
     'Chinatown',
     'Civic Center',
     'Clinton Hill',
     'Columbia Street Waterfront District',
     'Downtown Brooklyn',
     'Dumbo',
     'East Village',
     'Financial District',
     'Flatiron District',
     'Fort Greene',
     'Fulton Ferry District',
     'Garment District',
     'Gramercy Park',
     'Greenwich Village',
     "Hell's Kitchen",
     'Hudson Square',
     'Kips Bay',
     'Korea Town',
     'Lincoln Square',
     'Little Italy',
     'Lower East Side',
     'Lower Manhattan',
     'Meatpacking District',
     'Midtown',
     'Midtown East',
     'Midtown West',
     'Murray Hill',
     'NoHo',
     'NoMad',
     'Nolita',
     'Park Slope',
     'Peter Cooper Village',
     'Rose Hill',
     'SoHo',
     'Stuyvesant Town',
     'Sutton Place',
     'Theater District',
     'Tribeca',
     'Tudor City',
     'Two Bridges',
     'Ukrainian Village',
     'Union Square',
     'Vinegar Hill',
     'West Village',
     'Williamsburg']




```python
# yea so 54 in X and y... but i used stationsdf w/ 75 neighborhoods, 
# so at the very least I should not be using that if it ends up indeed , 
# blowing up the one hot encoder to 75 , ... this might be why we have those start neighborhoods, 
# with  0 importance, since there was just no data for them 

# Anyhow since the predict_proba output is indeed only length 54, that tells me the classifier
# ended up doing the rigiht thing anway. So I will keep this in mind for the next dataset but 
# in this case it is okay anyway.
```

### 2020-06-13

#### quick update, also just calculate a multiclass logloss as well
- Hmm don't have the indices I used in notebook "2020-06-10-again" so I have to do that again
- Also kernel died while trying to do this so numbers starting over 
- And wow happened twice. Turns out running log_loss() with 843416 rows did the trick!


```python
import pandas as pd
import xgboost as xgb
from xgboost import XGBClassifier
import datetime; import pytz
import matplotlib as plt
from scipy.special import softmax
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split # (*arrays, **options)
import numpy as np
from sklearn.metrics import log_loss
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

from joblib import dump, load
import joblib
import os
from sklearn.metrics import confusion_matrix, mean_squared_error
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
import fresh.utils as fu

from importlib import reload
from collections import Counter
from tqdm.notebook import tqdm
import fresh.preproc.v1 as pv1
```


```python
datadir = '/opt/data'
localdir = '/opt/program'
tripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv')
stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',
                        index_col=0)
```


```python
# load model from 2020-06-10 notebook
bundle = joblib.load('/opt/program/artifacts/2020-06-11T041641Z/bundle.joblib')
```


```python
X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)
len(neighborhoods)
```




    75




```python
labels = sorted(list(dict(Counter(y)).keys()))
```


```python
model = bundle['model']
```


```python
%%time
y_prob_vec = []
X_parts = fu.get_partitions(X, slice_size=1000)
for X_part in tqdm(X_parts):
    X_transformed = bundle['preproc']['one_hot_enc'].transform(X_part)
    y_prob = model.predict_proba(X_transformed)
    y_prob_vec.extend(y_prob)
# log_loss(y, y_prob, labels=sorted(list(dict(Counter(y)).keys())))
```

    100%|██████████| 844/844 [01:44<00:00,  8.07it/s]

    CPU times: user 6min 40s, sys: 4.94 s, total: 6min 44s
    Wall time: 1min 44s


    



```python
log_loss(y[:100], y_prob_vec[:100], labels=sorted(list(dict(Counter(y)).keys())))
```




    4.1337020254135135




```python
losses_vec = []
for part in fu.get_partitions(list(range(len(y_prob_vec))), slice_size=1000):
    i, j = part[0], part[-1]   
    losses_vec.append(log_loss(y[i:j], y_prob_vec[i:j], labels=labels))
```


```python
print(losses_vec)
```

    [3.9443905231830954, 4.042430092264582, 4.284518785543509, 4.162552323785272, 3.952456100566967, 4.03695215978422, 4.084479059900965, 3.4199061477267825, 3.272257596045524, 3.3829068352390936, 3.3745822951839015, 3.77494844803223, 3.7140628616134443, 3.8837221286914967, 4.124399999956469, 3.9265993123536593, 4.0768903036852615, 3.174890831784085, 2.1633148409344174, 2.182484753258355, 3.7437190295936347, 4.212761341511189, 4.193068556122117, 4.050813338420054, 4.265482315430054, 4.028255045950949, 4.287441587066269, 4.37707514949031, 3.9286252593134976, 3.5591859927287213, 3.4558437285838544, 3.707750113996061, 4.6341632608178855, 4.7137143411435884, 4.660421457615223, 3.8695895977325745, 3.1009091063662693, 3.0935432786817425, 3.0936311737553135, 3.0073481510590026, 2.96172045969271, 2.6241887278027005, 2.643367058641321, 2.6824022163977257, 2.659424974395706, 2.767015855114262, 3.494232348135642, 3.660266458093225, 3.6572457316878797, 3.403857082695336, 3.3098975440284035, 3.5316637483564346, 3.601163441712434, 3.630678286184897, 3.759934095529703, 3.5814467937022716, 3.0621027041961244, 2.817778690918549, 2.713518172890336, 3.274877048708178, 3.1641457644072144, 3.26192586486404, 2.373639445643764, 2.8746421905131907, 2.937836243702962, 2.815415211387344, 2.692828953683794, 2.6160492348122046, 2.7376425419006503, 3.1529374463899478, 2.856999825428914, 3.0661379816057206, 2.784436594497215, 2.733124007811179, 2.6841282070100725, 2.7526282320270785, 2.7169589971278882, 3.349861900131027, 3.553918444955194, 3.6813637950160243, 3.6054250649861745, 3.9874546282045595, 4.203858355024794, 4.056273417907195, 4.263221395624292, 4.125480605078651, 3.9996967894417628, 2.8837883748091735, 2.7697615832299203, 3.013261819148326, 3.051860484275016, 3.23851032419367, 3.7831092432096556, 3.7353892934930935, 3.8197124942286953, 4.122295409470826, 3.923378474242217, 3.1141727659198732, 3.2190994211622663, 3.4233330117092953, 3.2845175762672922, 2.1766693623096973, 2.0147515957777924, 2.0461544229222968, 2.0918168691066175, 2.385166830964036, 3.004038057050428, 2.9454414515165954, 3.2603233194685317, 3.5169942250122896, 3.713208071104399, 3.612622432403259, 4.760986496378352, 4.735360512623677, 4.407800625036429, 2.6370532590228395, 1.0107910045990356, 2.221192752157484, 2.0442487209050864, 1.908525362625733, 2.0454440354346275, 2.15475521681903, 3.893289122614894, 3.9223847324783736, 4.025913942802895, 3.683076638717193, 3.4966866697993004, 3.6147265009455256, 2.985785372622378, 2.808239019668854, 2.994529491191631, 2.8827640124866076, 3.0943161955586187, 2.9943869874045417, 3.206471788274633, 3.385732771040083, 3.441508171675322, 3.1034429292898396, 2.823096615893466, 2.6861368490530326, 2.870773143119163, 3.112235683578629, 3.2719866193689264, 3.2029021767166643, 3.356119539405968, 3.154938139834323, 2.110750208029876, 1.9841651120582022, 2.0817611898865187, 2.5279825873322435, 3.5373734375378034, 3.660636894934409, 2.4355496884108305, 1.9408807799861476, 2.02533872969993, 2.1429111874020017, 4.549958873200822, 5.497362258078696, 5.359373592876935, 3.718731479959803, 3.5770609106745446, 3.6032673515476383, 3.4049465298294663, 3.668425370503713, 3.801024261537615, 3.5668787290384105, 3.542309951734495, 3.602055324567808, 3.4696891707581683, 2.689433905813429, 2.6661047449818365, 2.775656614575658, 2.6261962423095473, 2.832787096321404, 2.8336101537710197, 3.0113465325371758, 3.0803157138872193, 3.0622415074834355, 2.5495392526830876, 2.5237403557942555, 2.386597220365469, 2.1479714414378903, 2.255386908133109, 3.6549300242950014, 3.6067754666249194, 2.6568563440301873, 2.7008794669035794, 2.7347124094242328, 2.6061925625538565, 1.9179738442580383, 1.990815867532839, 2.101627956997525, 4.383031890198991, 5.182109409863049, 5.241836977673245, 4.973378728698562, 3.459109682936568, 3.288043922370857, 3.3804774408464557, 4.250087378857969, 4.792045131221309, 4.801484761414705, 3.544068166324207, 2.943145388239497, 3.0386576549904243, 3.807079971731604, 3.743546917154505, 3.824494246367339, 3.868677289397628, 3.888452051876782, 4.0761355366912095, 4.32434336606924, 4.4962236666464594, 3.617289082304732, 2.7073031659121507, 3.2125261621074275, 3.330107056223475, 3.3469210281983033, 3.25832265335041, 2.7201443236392064, 3.5091409806136014, 3.832654969708936, 3.7429192944451257, 3.8295228693697667, 3.89154456446956, 3.9955779697086, 3.86314450011955, 4.1104246013992665, 4.007098416069725, 3.501023805654562, 3.361241976897399, 3.3685810766420565, 3.3444653979770176, 3.6098379119857773, 3.704393203552063, 3.250827876416532, 2.912101593342152, 3.0634451449454367, 2.981495338636595, 3.051914754691902, 3.054172988649126, 3.03183581282546, 3.1169798808532194, 2.6607681549824513, 2.154008444842395, 2.1365581746096605, 1.926606001558008, 2.304765788284508, 2.105576227019141, 2.2524976034660837, 2.3688968380650244, 2.1798694452365956, 2.1646925147708593, 2.054496369204364, 2.1020746567585804, 2.280715175219126, 2.5289561984775304, 2.033951759099722, 1.9591198833855066, 2.023693098439588, 2.1634568869769275, 3.327817655301786, 3.511018919873166, 2.894801412616764, 2.9852163879959672, 3.3834276927245392, 4.314223944484532, 4.208546276326413, 3.1939467176183447, 4.253436582105176, 4.330566325583854, 3.9472611511314475, 3.228095275623066, 2.9326886777524597, 3.097231464701014, 3.0951773485502563, 3.3785032774951005, 3.5376334297764407, 3.495245829716817, 3.5346177977484627, 3.1748212763258405, 2.1448168410911217, 2.2587558059721022, 2.165229523981417, 2.519093419219161, 3.3145281397425257, 3.4119785674460776, 3.5003824162411616, 3.260227775788522, 2.5776322485806347, 3.561216638491557, 3.615262298373966, 3.6803919099591993, 3.7214751055052093, 3.717513544542773, 3.690337783223516, 2.857362224294378, 2.869904929453188, 4.566323933062014, 4.753674617878071, 4.884940527342223, 2.914246288267103, 2.710325808019132, 2.704564550736764, 2.8944299975195684, 2.8241396759365416, 2.6063153940636115, 2.6784340085925997, 2.574543732422608, 2.603323427764503, 3.4846620247051403, 3.5998747446634867, 3.3697594111865468, 3.481394575164841, 3.4510105100121944, 3.174058922656902, 2.681280468796586, 2.661300646292197, 2.842615399394069, 2.8565139016350947, 2.956626638158544, 3.0070692750665398, 3.093308107511656, 2.65285183860733, 3.096826838540124, 3.002715519359997, 3.8626401916041866, 3.9840526416137054, 3.9554031606431717, 3.9457726705301033, 3.0376922456590503, 2.8629410343246535, 3.285067499340237, 4.117502545451258, 4.231143174586712, 3.90684763399569, 3.981921616497937, 3.973033455876378, 3.2937680319384173, 3.3699267168302796, 3.336427587646622, 4.058913602246656, 4.142137223416501, 3.4022700374906845, 2.9579908674782343, 2.6149111267802, 2.6962649976169026, 2.8239888440858616, 3.590625005441385, 3.799182539348965, 3.774379767932453, 3.8243985462475107, 3.8341419622824118, 3.9502420260741546, 3.2962471974623933, 2.5758594058536075, 2.012419293234656, 1.9668361003453787, 1.985346136269746, 2.056992055775525, 2.1342515631600305, 2.2671280976410983, 3.3834231736304403, 3.774456751597178, 2.8120592714429975, 2.913241187134782, 2.832235963136942, 2.8301808425972053, 2.711992226205431, 2.7581846754591504, 3.265864474160058, 4.156752958312049, 4.04413311569779, 3.03062399479004, 3.876103262762885, 3.987023924922084, 3.3010726163575836, 2.7652102850817584, 2.7349393348674753, 3.029874228858375, 3.0195755400099196, 3.3294311297667756, 3.538807129120087, 3.706613750667782, 3.6558824385011994, 3.6653498863434053, 3.625982612222284, 3.656551553680374, 3.0586499165486285, 2.810931305030922, 2.9675692232760102, 3.7742969648496763, 4.158406973601103, 2.2621753265907816, 1.854672708430209, 1.997093751265838, 2.109133343319516, 2.784471834505404, 2.8059545393820637, 2.907624017368924, 2.913719840355225, 2.9305215194537952, 3.4000282962997637, 3.405927512022826, 3.4407760906028555, 3.2911672974014663, 2.9246809079243734, 2.825256712324507, 3.14065229868865, 3.665093107624455, 3.1786604352422185, 3.138268159555124, 3.7315089848187117, 4.448839778776045, 5.17461531871074, 5.142818393172683, 3.897681892097175, 3.8381576428303608, 3.9252068726746767, 4.083763543072644, 4.018973688224892, 4.830412697386336, 5.100182097953361, 3.945646909144786, 4.118985179904942, 3.8666223584710657, 2.918752205861104, 2.984236981894996, 2.9554198890834957, 3.0121189211462593, 3.067869920272369, 3.065411724008478, 2.6527953437856726, 2.987812376953102, 3.9365520291142277, 4.064694794806632, 3.906747579574585, 3.3855226317922154, 3.4397981731263965, 3.2512133347021566, 2.5264420932954974, 2.011601136134074, 2.068679784391974, 2.125459980678272, 2.854908775280904, 3.0687738680863403, 3.0687978120895476, 3.5691839190455408, 3.33353662037396, 3.4560722247496978, 3.0972121769004874, 3.0838496192200884, 2.2693811830457626, 2.0222285016998276, 2.1077667339905366, 2.933366280179601, 3.405730945331318, 2.9633400950942552, 2.8811909448396458, 2.8070906891121163, 3.003884656054599, 1.993785086814109, 1.9276828399530284, 2.216680566231171, 3.8561905731071344, 3.942095554149425, 3.843687810697355, 3.931914813525684, 4.030184065376793, 4.037149241259387, 4.0175599433757645, 3.7638716155940943, 2.9374291062713027, 3.5030754686475873, 3.3567864328056007, 3.603965678849855, 2.8189743264420732, 3.3409990575817137, 3.320795654653906, 3.1917507084043653, 3.4518987220806165, 3.414386812034431, 3.12705066516712, 2.9100114778952078, 3.103439383081965, 3.0150630450224853, 3.0743978985317715, 2.976726489024119, 3.470956861793816, 3.3470263791394546, 2.8779747432416625, 3.086039893977993, 3.0222249525087372, 2.7967211084442214, 3.840533847446079, 4.346148230530717, 4.082217499539182, 3.9426984464800037, 3.9155646630593606, 5.335169405073255, 4.759471056816934, 2.8949429029458993, 2.648106956625128, 3.0726859667399027, 3.1359508316796103, 3.265416428491518, 3.2162773085308745, 3.2596569015934422, 3.4040221284936973, 3.401659913249202, 3.3962373750226513, 4.396797117886242, 5.347470979194145, 5.537777926470782, 3.964348564038167, 2.676496303117311, 2.5287192617212093, 2.7460084687004813, 4.5622815255288245, 4.697101810672978, 4.767404221676968, 3.0250205423261547, 2.541255223858464, 3.500740448872487, 3.9421055185663567, 3.935567199766218, 4.011089715633068, 3.357676337073157, 2.7002955962707094, 3.13370554988926, 3.4203406571148633, 3.4968888991110556, 3.249024221250364, 3.260680138885796, 2.812299950225456, 3.81161933606332, 4.0134960441617995, 3.856799130683189, 3.8423200639279873, 3.9610774342839545, 4.055627014066602, 3.9169453901571556, 4.003606061439018, 3.4380365052858033, 2.688100771025733, 2.7665992264036423, 2.806362538485675, 3.3835935036580005, 4.324600036915119, 4.006808256124471, 4.06497178182707, 3.624833541112142, 5.184718793576902, 5.216830265533936, 5.223427155831674, 4.520995015019292, 2.758503790493603, 2.9239166680279673, 2.974805451489545, 3.368994215467909, 3.7913922268349127, 3.771049928140115, 3.7868990924384622, 3.8877762523857324, 3.855616168574886, 3.6489611618273012, 2.934315372635056, 2.6478784695282593, 3.1421868560073136, 3.137532043027448, 3.0641325691918113, 3.014242506599999, 2.919869510022489, 3.264536979320171, 3.856037245617734, 3.9501368565125032, 3.9461429651315743, 3.9212186720755486, 3.8868375993944384, 3.7873999957923776, 3.9088862438698313, 3.909846323030489, 4.183663952458012, 4.084135966496663, 3.786912390658328, 3.034923843435339, 3.0450954301221236, 3.472830565722736, 3.5310496999933436, 3.4302671669243097, 3.4367839418970667, 3.137437390135573, 3.1019590280435465, 2.8117921923970557, 2.086361154541, 2.1317731389054306, 2.5159099136625565, 2.950429553742165, 2.749888650051228, 2.135234546971631, 2.032585230794874, 2.074969659099827, 1.9501734393256325, 2.0627395292898796, 2.0660096385457494, 2.3891353279024035, 2.6156848362377576, 3.048549081589486, 2.836415475075906, 2.511387685875038, 3.8757489842098876, 4.026702767258531, 3.592280121059628, 2.7813268694672377, 1.9412666592631374, 2.0517023738559423, 2.1587233093527107, 3.9283449754819975, 4.2009758383662135, 3.9523552153800225, 3.8282500406404636, 3.86913693416584, 3.6909821422489077, 3.1683803000846305, 2.8748271775555923, 2.963243130091074, 3.373841147761684, 3.6936527453623973, 3.684498089331168, 3.9277215415651017, 3.1462134159840383, 2.2498991700622053, 1.9281181024240182, 2.0747447975643643, 2.4410663657002263, 2.786135860152908, 2.945836582222023, 2.0417774755794844, 1.888119992193159, 2.038727311042694, 2.7131580660889694, 4.223705857843012, 3.708216523622965, 2.7736716814585276, 3.214771067296659, 3.3253736481652245, 3.3149433677738256, 2.8923214832703033, 3.151243671521291, 3.5618177252608136, 2.807373324433366, 2.9767705413314314, 3.3051647027810893, 2.9562306683342734, 4.15621300096865, 4.15508793567394, 4.299649605402598, 2.3351480051561877, 2.0472937317581863, 2.1838903563158647, 3.7418471389823966, 3.332467790361162, 2.9171562521784633, 3.0024668867284947, 2.915093187574629, 3.030600360444597, 2.8521818152896397, 3.0188346751578696, 2.7880476480489738, 2.9536679330411495, 3.0501159972495384, 2.8361843906723343, 2.700349591038487, 2.9999649006802516, 3.1103380931628957, 2.084366989684654, 1.9192837593672392, 2.068037249185182, 4.501865059047848, 2.991407569344934, 2.9874390157254727, 2.8859394454621934, 2.794149838529669, 3.715637996748045, 3.995096868700213, 3.1203497668525, 2.949571746724981, 2.8028481155783087, 2.416884018971517, 3.515780652368868, 3.311412448400969, 3.075803382737978, 2.997446676154991, 3.0114771310273594, 2.9562217812161067, 3.181450491314297, 3.4389754210387147, 3.634459742316016, 3.8999104103645883, 3.895587000641618, 3.638417751342804, 3.1287508661205226, 3.1294034005882025, 2.828361616120324, 2.947604368279527, 3.1396901961919426, 2.6583961390398883, 2.696208207456915, 2.6895239680140346, 2.774275873755072, 2.753040079359297, 2.7776308253004744, 2.691373447994809, 3.389002625290696, 4.176848485782459, 4.140449596954896, 4.160382177020694, 4.054090550950578, 2.8275210893189944, 3.616446994565748, 3.7231787098301306, 3.790569509233202, 3.989031739659734, 4.487514716130239, 4.444086919437061, 4.377709531449938, 3.9184411693741015, 3.065334384863799, 3.5744309347790404, 3.263745908980613, 3.946962528400593, 3.5243246785632603, 2.6112481674274526, 4.1146269148892465, 4.367964908286735, 4.638424821802087, 4.508027511554676, 4.556052026805935, 3.538557225638801, 4.322172033655512, 3.689250429948648, 3.4156787038684726, 3.371513792583057, 3.037871669720601, 2.567961732785146, 3.9342438204510435, 3.726187363043204, 3.774534925445541, 3.6066227868512586, 2.658271553160789, 3.9270670511820414, 4.64282024921001, 4.28593467257999, 4.408086886038412, 4.310798121405555, 4.236416566598642, 4.3554603993833005, 3.671090945705876, 2.899425573773809, 2.756461577372508, 2.7112024897211664, 3.160116480396794, 3.267217214401062, 3.0240364151077346, 2.9301366596011906, 2.984034308203467, 3.045494527788134, 3.063485752712857, 2.633197175251232, 2.785610773899892, 4.270052742194365, 4.28326791399592, 4.3432301906971365, 4.3797861954590696, 4.253429864619945, 3.7164177644002185, 3.239922128162823, 3.3866417456914237, 3.106457813366039, 2.786826659131933, 2.874356944401104, 3.076688885569453, 2.9185086038854866, 2.7797276817403875, 2.9236848014253036, 3.0354371918095007, 3.0162612043462835, 2.9181266183013075, 2.7449238147582853, 2.7886331539612277, 4.357319263963251, 3.845828733644686, 3.8933344841958046, 3.2915042422793888, 4.4452638460231855, 2.793156823954425, 2.8687252191690593, 4.028857362640274, 4.307344119470041, 3.3914541682681523, 3.5303030849338413, 3.838527156187369, 4.5352281581412806, 2.8847034577731496, 2.5686143739326104, 3.168579631500893, 3.145843793083359, 3.8597748791491306, 4.233597641234641, 4.291998183047092, 4.475943486850421, 3.684890280018101, 3.1024442168924065, 3.2928745201280765, 3.941358158418963, 3.6424733139015175, 2.9654302735467097, 2.763859815783687, 2.5145508704839408, 3.6049983352273554, 4.04043942099219, 3.658149067823355, 2.716333639752996, 3.300562561572612, 3.1599973218219057, 3.5417246597784535, 3.4623895872820607, 3.4742063678898014, 3.138214178510137, 3.3415376999237396, 3.914780088969776, 2.7470618628405474, 2.6970778521116787, 3.3003058316590668, 3.663887528685836, 4.204170827751045, 3.3093152392256604, 3.4908265213111975, 3.2294646925157733, 2.9868477691520563, 2.803750857099279, 3.3944816462867133, 3.181306352128496, 3.468752476967961]



```python
# Average the separate log loss measurements
# This number of course does not matter because it includes the training data. Just testing this out.
np.mean(losses_vec)
```




    3.305831202815704




```python
labels[:10]
```




    ['Alphabet City',
     'Battery Park City',
     'Bedford-Stuyvesant',
     'Boerum Hill',
     'Bowery',
     'Brooklyn Heights',
     'Brooklyn Navy Yard',
     'Central Park',
     'Chelsea',
     'Chinatown']




```python
%%time
fu.big_logloss(y, y_prob=y_prob_vec, labels=labels)
```

    CPU times: user 2.96 s, sys: 20 ms, total: 2.98 s
    Wall time: 2.99 s





    3.305831202815704




```python
workdir = fu.make_work_dir()
```


```python
joblib.dump({'y_prob_vec': y_prob_vec}, f'{workdir}/blah.joblib')
```




    ['/opt/program/artifacts/2020-06-13T223758Z/blah.joblib']




```python
! ls -lah /opt/program/artifacts/2020-06-13T223758Z/blah.joblib
```

    -rw-r--r-- 1 root root 209M Jun 13 22:42 /opt/program/artifacts/2020-06-13T223758Z/blah.joblib


#### Another go
But store the intermediary results this time unlike inthe 2020-06-10 notebook


```python
X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)
len(neighborhoods)
```




    75




```python
labels = sorted(list(dict(Counter(y)).keys()))
```


```python
1
```




    1




```python
%%time
rng = np.random.RandomState(31337)
X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)
neighborhoods = sorted(stationsdf.neighborhood.unique().tolist())

X, y = X[:10000], y[:10000]

kf = KFold(n_splits=5, shuffle=True, random_state=rng)
for i, (train_index, test_index) in enumerate(tqdm(kf.split(X), desc='outer', leave=True)):
    
    # preprocess
    (X_transformed, one_hot_enc, le,
         y_enc) = pv1.preprocess(X[train_index], y[train_index], 
                             neighborhoods)
    
    xgb_model = xgb.XGBClassifier().fit(X_transformed, y_enc)
    
    X_test_transformed = one_hot_enc.transform(X[test_index]).toarray()
    actuals = le.transform(y[test_index])
    
    predictions = xgb_model.predict(X_test_transformed)
    correct = len([i for i, _ in enumerate(actuals)
              if actuals[i] == predictions[i]])

    y_prob_vec = []
    X_parts = fu.get_partitions(X_test_transformed, slice_size=1000)
    for X_part in tqdm(X_parts, desc='inner', leave=False):
        # X_transformed = one_hot_enc.transform(X_part)
        y_prob = model.predict_proba(X_part)
        y_prob_vec.extend(y_prob)

    # y_prob_vec = xgb_model.predict_proba(X_test_transformed)
    
    joblib.dump({'loss': fu.big_logloss(actuals, y_prob=y_prob_vec, labels=labels),
     'confusion_matrix': confusion_matrix(actuals, predictions),
     'model': xgb_model,
     'notebook': '2020-06-12.ipynb',
     'accuracy': correct/len(actuals),
     'timestamp': fu.utc_ts(),
    }, f'{workdir}/bundle_{i}.joblib')
```


```python
X.shape
```




    (843416, 2)




```python
correct/len(actuals)
```




    0.15452562187285102




```python
y_prob_vec
```




    []




```python
type(np.ndarray)
```




    type




```python
    X_parts = fu.get_partitions(X_test_transformed, slice_size=1000)

```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-27-1363a7b0d258> in <module>
    ----> 1 X_parts = fu.get_partitions(X_test_transformed, slice_size=1000)
    

    /opt/program/fresh/utils.py in get_partitions(vec, slice_size, keep_remainder)
         39 def get_partitions(vec, slice_size, keep_remainder=True):
         40     assert slice_size > 0
    ---> 41     num_slices = int(math.floor(len(vec)/slice_size))
         42     size_remainder = len(vec) - num_slices*slice_size
         43     assert size_remainder >= 0


    /opt/conda/lib/python3.7/site-packages/scipy/sparse/base.py in __len__(self)
        293     # non-zeros is more important.  For now, raise an exception!
        294     def __len__(self):
    --> 295         raise TypeError("sparse matrix length is ambiguous; use getnnz()"
        296                         " or shape[0]")
        297 


    TypeError: sparse matrix length is ambiguous; use getnnz() or shape[0]



```python
# 0:45 , starting 
# 1:00 ,  still going, but dang no sign of the inner tqdm yet (the predict proba)
# 1:01 => TypeError, sparse matrix length is ambiguous; use getnnz() or shape[0]
#          
#/opt/program/fresh/utils.py in get_partitions(vec, slice_size, keep_remainder)
#      39 def get_partitions(vec, slice_size, keep_remainder=True):
#      40     assert slice_size > 0
# ---> 41     num_slices = int(math.floor(len(vec)/slice_size))
```


```python
from tqdm.notebook import tqdm
import time
for i in tqdm(range(10), desc='outer', leave=True):
    for j in tqdm(range(10), desc='inner', leave=False):
        time.sleep(.5)

```


    HBox(children=(FloatProgress(value=0.0, description='outer', max=10.0, style=ProgressStyle(description_width='…



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo



    HBox(children=(FloatProgress(value=0.0, description='inner', max=10.0, style=ProgressStyle(description_width='…


    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    foo
    



```python

```


```python
# pretty sure since I had had to partition the logloss , I can still average it to get a full score.
i, j = 0, 1000
a = log_loss(y[i:j], y_prob_vec[i:j], labels=labels)

i, j = 1000, 2000
b = log_loss(y[i:j], y_prob_vec[i:j], labels=labels)

i, j = 0, 2000
c = log_loss(y[i:j], y_prob_vec[i:j], labels=labels)

(a+b)/2, c

```




    (3.993434763431549, 3.9934347634315492)


