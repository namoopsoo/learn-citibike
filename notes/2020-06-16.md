
- Similar to end of 2020-06-14 , but I want to try many iterations, still batching to try avoid 
kernel exploding

- This time... going to just perhaps, shrink the test set and only test after every iteration..

- In the 2020-06-14 notebook, ran through 17 out of the 633 or so batches I had setup per epoch. And that took the whole night when I slept  (2020-06-16 03:57:33Z to  16:20:23Z  ) 
- During that time, the accuracy did not budge really. (The logloss was 0 for some reason. I still have not debugged that ). Got to debug that 0 logloss. 


```python
import pandas as pd
import xgboost as xgb
from xgboost import XGBClassifier
import datetime; import pytz
import matplotlib as plt
from scipy.special import softmax
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split # (*arrays, **options)
import numpy as np
from sklearn.metrics import log_loss
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

from joblib import dump, load
import joblib
import os
from sklearn.metrics import confusion_matrix, mean_squared_error
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
import fresh.utils as fu

from importlib import reload
from collections import Counter
from tqdm.notebook import tqdm
import fresh.preproc.v1 as pv1
```


```python
datadir = '/opt/data'
localdir = '/opt/program'

# This time only take about a 1/60th  of the data... 0.017% ... 
tripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv'
                     ).sample(frac=0.017, random_state=42)
stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',
                        index_col=0)

```


```python
X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)
X_train, X_test, y_train, y_test = train_test_split(X, y)

# preproc
(X_transformed,
     one_hot_enc, le,
     y_enc) = pv1.preprocess(X_train, y_train, # X[train_index]
                         neighborhoods)
labels = le.classes_

# Test set
X_test_transformed = one_hot_enc.transform(X_test)
y_test_enc = le.transform(y_test)
```


```python

size = X_transformed.shape[0]
print(size)
indices = np.random.choice(range(size), size=size, replace=False)

parts = fu.get_partitions(indices, slice_size=1000)
len(parts[0])
len(Counter(y_enc[parts[0]]))
```

    10753





    54




```python
y_test_enc.shape
```




    (3585,)




```python
len(parts)
```




    11




```python
# This time... going to just perhaps, shrink the test set and only test after every iteration..
```


```python
workdir = fu.make_work_dir(); print(workdir)
fu.log(workdir, 'Starting')
prev_model = None
loss_vec = []; acc_vec = []

clf = xgb.XGBClassifier()

for epoch in tqdm(range(100), desc='epochs'):
    fu.log(workdir, f'[{epoch}:{i}] Done fit')
    for i, part in enumerate(tqdm(parts, leave=False, desc='batches')):
        clf.fit(X_transformed[part], y_enc[part], xgb_model=prev_model)
        fu.log(workdir, f'[{epoch}:{i}] Done fit')

        prev_model = f'{workdir}/model.xg'
        clf.save_model(prev_model)

    y_prob_vec = clf.predict_proba(X_test_transformed)
    fu.log(workdir, f'[{epoch}] Done predict_proba')

    loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
    fu.log(workdir, f'[{epoch}] Done big_logloss, loss={loss}.')

    loss_vec.append(loss)

    acc = accuracy_score(y_test_enc, np.argmax(y_prob_vec, axis=1))
    acc_vec.append(acc)
    fu.log(workdir, f'[{epoch}] Done accuracy, acc={acc}.')
    
    
```

    /opt/program/artifacts/2020-06-17T042025Z



    HBox(children=(FloatProgress(value=0.0, description='epochs', style=ProgressStyle(description_width='initial')…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…



    HBox(children=(FloatProgress(value=0.0, description='batches', max=11.0, style=ProgressStyle(description_width…


    



    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-32-0c7b14cbe3bb> in <module>
          9     fu.log(workdir, f'[{epoch}:{i}] Done fit')
         10     for i, part in enumerate(tqdm(parts, leave=False, desc='batches')):
    ---> 11         clf.fit(X_transformed[part], y_enc[part], xgb_model=prev_model)
         12         fu.log(workdir, f'[{epoch}:{i}] Done fit')
         13 


    /opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py in fit(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)
        730                               evals_result=evals_result, obj=obj, feval=feval,
        731                               verbose_eval=verbose, xgb_model=xgb_model,
    --> 732                               callbacks=callbacks)
        733 
        734         self.objective = xgb_options["objective"]


    /opt/conda/lib/python3.7/site-packages/xgboost/training.py in train(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)
        214                            evals=evals,
        215                            obj=obj, feval=feval,
    --> 216                            xgb_model=xgb_model, callbacks=callbacks)
        217 
        218 


    /opt/conda/lib/python3.7/site-packages/xgboost/training.py in _train_internal(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)
         72         # Skip the first update if it is a recovery step.
         73         if version % 2 == 0:
    ---> 74             bst.update(dtrain, i, obj)
         75             bst.save_rabit_checkpoint()
         76             version += 1


    /opt/conda/lib/python3.7/site-packages/xgboost/core.py in update(self, dtrain, iteration, fobj)
       1107         if fobj is None:
       1108             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),
    -> 1109                                                     dtrain.handle))
       1110         else:
       1111             pred = self.predict(dtrain)


    KeyboardInterrupt: 


- As this is running, I'm definitely wondering if different forms of batching deteriorates the ability to learn.
- Or perhaps is that just a parameter tuning problem?

- I killed this loop because not much progress was made after `13` iterations

```
(pandars3) $ tail -f artifacts/2020-06-17T042025Z/work.log 
2020-06-17 04:20:25Z, Starting
2020-06-17 04:20:25Z, [0:3] Done fit
2020-06-17 04:20:27Z, [0:0] Done fit
2020-06-17 04:20:30Z, [0:1] Done fit
2020-06-17 04:20:34Z, [0:2] Done fit
2020-06-17 04:20:40Z, [0:3] Done fit
2020-06-17 04:20:51Z, [0:4] Done fit
2020-06-17 04:21:04Z, [0:5] Done fit
2020-06-17 04:21:20Z, [0:6] Done fit
2020-06-17 04:21:39Z, [0:7] Done fit
2020-06-17 04:22:02Z, [0:8] Done fit
2020-06-17 04:22:30Z, [0:9] Done fit
2020-06-17 04:22:53Z, [0:10] Done fit
2020-06-17 04:23:47Z, [0] Done predict_proba
2020-06-17 04:23:47Z, [0] Done big_logloss, loss=0.0.
2020-06-17 04:23:47Z, [0] Done accuracy, acc=0.023709902370990237.
2020-06-17 04:23:47Z, [1:10] Done fit
2020-06-17 04:24:17Z, [1:0] Done fit
2020-06-17 04:24:52Z, [1:1] Done fit
2020-06-17 04:25:30Z, [1:2] Done fit
2020-06-17 04:26:12Z, [1:3] Done fit
2020-06-17 04:26:56Z, [1:4] Done fit
2020-06-17 04:27:44Z, [1:5] Done fit
2020-06-17 04:28:35Z, [1:6] Done fit
2020-06-17 04:29:30Z, [1:7] Done fit
2020-06-17 04:30:30Z, [1:8] Done fit
2020-06-17 04:31:31Z, [1:9] Done fit
2020-06-17 04:32:22Z, [1:10] Done fit
2020-06-17 04:34:16Z, [1] Done predict_proba
2020-06-17 04:34:16Z, [1] Done big_logloss, loss=0.0.
2020-06-17 04:34:16Z, [1] Done accuracy, acc=0.021757322175732216.
2020-06-17 04:34:16Z, [2:10] Done fit
2020-06-17 04:35:21Z, [2:0] Done fit
2020-06-17 04:36:32Z, [2:1] Done fit
2020-06-17 04:37:47Z, [2:2] Done fit
2020-06-17 04:39:05Z, [2:3] Done fit
2020-06-17 04:40:26Z, [2:4] Done fit
2020-06-17 04:41:50Z, [2:5] Done fit
2020-06-17 04:43:18Z, [2:6] Done fit
2020-06-17 04:44:49Z, [2:7] Done fit
2020-06-17 04:46:23Z, [2:8] Done fit
2020-06-17 04:48:01Z, [2:9] Done fit
2020-06-17 04:49:18Z, [2:10] Done fit
2020-06-17 04:52:11Z, [2] Done predict_proba
2020-06-17 04:52:11Z, [2] Done big_logloss, loss=0.0.
2020-06-17 04:52:11Z, [2] Done accuracy, acc=0.017573221757322177.
2020-06-17 04:52:11Z, [3:10] Done fit
2020-06-17 04:53:48Z, [3:0] Done fit
2020-06-17 04:55:33Z, [3:1] Done fit
2020-06-17 04:57:22Z, [3:2] Done fit
2020-06-17 04:59:12Z, [3:3] Done fit
2020-06-17 05:01:06Z, [3:4] Done fit
2020-06-17 05:03:03Z, [3:5] Done fit
2020-06-17 05:05:05Z, [3:6] Done fit
2020-06-17 05:07:07Z, [3:7] Done fit
2020-06-17 05:09:15Z, [3:8] Done fit
2020-06-17 05:11:26Z, [3:9] Done fit
2020-06-17 05:13:10Z, [3:10] Done fit
2020-06-17 05:17:04Z, [3] Done predict_proba
2020-06-17 05:17:04Z, [3] Done big_logloss, loss=0.0.
2020-06-17 05:17:04Z, [3] Done accuracy, acc=0.01701534170153417.
2020-06-17 05:17:04Z, [4:10] Done fit
2020-06-17 05:19:14Z, [4:0] Done fit
2020-06-17 05:21:35Z, [4:1] Done fit
2020-06-17 05:23:59Z, [4:2] Done fit
2020-06-17 05:26:27Z, [4:3] Done fit
2020-06-17 05:28:57Z, [4:4] Done fit
2020-06-17 05:31:33Z, [4:5] Done fit
2020-06-17 05:34:11Z, [4:6] Done fit
2020-06-17 05:36:55Z, [4:7] Done fit
2020-06-17 05:39:37Z, [4:8] Done fit
2020-06-17 05:42:26Z, [4:9] Done fit
2020-06-17 05:44:36Z, [4:10] Done fit
2020-06-17 05:49:28Z, [4] Done predict_proba
2020-06-17 05:49:28Z, [4] Done big_logloss, loss=0.0.
2020-06-17 05:49:28Z, [4] Done accuracy, acc=0.01785216178521618.
2020-06-17 05:49:28Z, [5:10] Done fit
2020-06-17 05:52:17Z, [5:0] Done fit
2020-06-17 05:55:13Z, [5:1] Done fit
2020-06-17 05:58:15Z, [5:2] Done fit
2020-06-17 06:01:17Z, [5:3] Done fit
2020-06-17 06:04:28Z, [5:4] Done fit
2020-06-17 06:07:37Z, [5:5] Done fit
2020-06-17 06:10:55Z, [5:6] Done fit
2020-06-17 06:14:13Z, [5:7] Done fit
2020-06-17 06:17:36Z, [5:8] Done fit
2020-06-17 06:21:02Z, [5:9] Done fit
2020-06-17 06:23:46Z, [5:10] Done fit
2020-06-17 06:29:54Z, [5] Done predict_proba
2020-06-17 06:29:54Z, [5] Done big_logloss, loss=0.0.
2020-06-17 06:29:54Z, [5] Done accuracy, acc=0.01896792189679219.
2020-06-17 06:29:54Z, [6:10] Done fit
2020-06-17 06:33:14Z, [6:0] Done fit
2020-06-17 06:36:56Z, [6:1] Done fit
2020-06-17 06:40:33Z, [6:2] Done fit
2020-06-17 06:44:19Z, [6:3] Done fit
2020-06-17 06:48:04Z, [6:4] Done fit
2020-06-17 06:52:00Z, [6:5] Done fit
2020-06-17 06:55:47Z, [6:6] Done fit
2020-06-17 06:59:48Z, [6:7] Done fit
2020-06-17 07:03:43Z, [6:8] Done fit
2020-06-17 07:07:49Z, [6:9] Done fit
2020-06-17 07:10:57Z, [6:10] Done fit
2020-06-17 07:17:57Z, [6] Done predict_proba
2020-06-17 07:17:57Z, [6] Done big_logloss, loss=0.0.
2020-06-17 07:17:57Z, [6] Done accuracy, acc=0.01785216178521618.
2020-06-17 07:17:57Z, [7:10] Done fit
2020-06-17 07:22:02Z, [7:0] Done fit
2020-06-17 07:26:15Z, [7:1] Done fit
2020-06-17 07:30:39Z, [7:2] Done fit
2020-06-17 07:34:58Z, [7:3] Done fit
2020-06-17 07:39:25Z, [7:4] Done fit
2020-06-17 07:43:49Z, [7:5] Done fit
2020-06-17 07:48:24Z, [7:6] Done fit
2020-06-17 07:52:55Z, [7:7] Done fit
2020-06-17 07:57:46Z, [7:8] Done fit
2020-06-17 08:02:24Z, [7:9] Done fit
2020-06-17 08:06:09Z, [7:10] Done fit
2020-06-17 08:14:36Z, [7] Done predict_proba
2020-06-17 08:14:36Z, [7] Done big_logloss, loss=0.0.
2020-06-17 08:14:36Z, [7] Done accuracy, acc=0.0200836820083682.
2020-06-17 08:14:36Z, [8:10] Done fit
2020-06-17 08:19:16Z, [8:0] Done fit
2020-06-17 08:24:16Z, [8:1] Done fit
2020-06-17 08:29:08Z, [8:2] Done fit
2020-06-17 08:34:17Z, [8:3] Done fit
2020-06-17 08:39:15Z, [8:4] Done fit
2020-06-17 08:44:27Z, [8:5] Done fit
2020-06-17 08:49:34Z, [8:6] Done fit
2020-06-17 08:55:00Z, [8:7] Done fit
2020-06-17 09:00:16Z, [8:8] Done fit
2020-06-17 09:05:48Z, [8:9] Done fit
2020-06-17 09:09:58Z, [8:10] Done fit
2020-06-17 09:19:24Z, [8] Done predict_proba
2020-06-17 09:19:24Z, [8] Done big_logloss, loss=0.0.
2020-06-17 09:19:24Z, [8] Done accuracy, acc=0.018688981868898186.
2020-06-17 09:19:24Z, [9:10] Done fit
2020-06-17 09:24:53Z, [9:0] Done fit
2020-06-17 09:30:24Z, [9:1] Done fit
2020-06-17 09:36:14Z, [9:2] Done fit
2020-06-17 09:41:58Z, [9:3] Done fit
2020-06-17 09:47:58Z, [9:4] Done fit
2020-06-17 09:53:45Z, [9:5] Done fit
2020-06-17 09:59:56Z, [9:6] Done fit
2020-06-17 10:05:56Z, [9:7] Done fit
2020-06-17 10:12:09Z, [9:8] Done fit
2020-06-17 10:18:20Z, [9:9] Done fit
2020-06-17 10:23:23Z, [9:10] Done fit
2020-06-17 10:34:41Z, [9] Done predict_proba
2020-06-17 10:34:41Z, [9] Done big_logloss, loss=0.0.
2020-06-17 10:34:41Z, [9] Done accuracy, acc=0.02203626220362622.
2020-06-17 10:34:41Z, [10:10] Done fit
2020-06-17 10:40:48Z, [10:0] Done fit
2020-06-17 10:47:31Z, [10:1] Done fit
2020-06-17 10:53:59Z, [10:2] Done fit
2020-06-17 11:00:42Z, [10:3] Done fit
2020-06-17 11:07:12Z, [10:4] Done fit
2020-06-17 11:14:02Z, [10:5] Done fit
2020-06-17 11:20:43Z, [10:6] Done fit
2020-06-17 11:27:41Z, [10:7] Done fit
2020-06-17 11:34:28Z, [10:8] Done fit
2020-06-17 11:41:37Z, [10:9] Done fit
2020-06-17 11:46:58Z, [10:10] Done fit
2020-06-17 11:59:06Z, [10] Done predict_proba
2020-06-17 11:59:06Z, [10] Done big_logloss, loss=0.0.
2020-06-17 11:59:06Z, [10] Done accuracy, acc=0.020362622036262202.
2020-06-17 11:59:06Z, [11:10] Done fit
2020-06-17 12:06:10Z, [11:0] Done fit
2020-06-17 12:13:15Z, [11:1] Done fit
2020-06-17 12:20:48Z, [11:2] Done fit
2020-06-17 12:28:00Z, [11:3] Done fit
2020-06-17 12:35:38Z, [11:4] Done fit
2020-06-17 12:43:03Z, [11:5] Done fit
2020-06-17 12:50:55Z, [11:6] Done fit
2020-06-17 12:58:28Z, [11:7] Done fit
2020-06-17 13:06:20Z, [11:8] Done fit
2020-06-17 13:13:58Z, [11:9] Done fit
2020-06-17 13:20:14Z, [11:10] Done fit
2020-06-17 13:34:20Z, [11] Done predict_proba
2020-06-17 13:34:20Z, [11] Done big_logloss, loss=0.0.
2020-06-17 13:34:20Z, [11] Done accuracy, acc=0.01896792189679219.
2020-06-17 13:34:20Z, [12:10] Done fit
2020-06-17 13:41:56Z, [12:0] Done fit
2020-06-17 13:50:16Z, [12:1] Done fit
2020-06-17 13:58:13Z, [12:2] Done fit
2020-06-17 14:06:39Z, [12:3] Done fit
2020-06-17 14:14:49Z, [12:4] Done fit
2020-06-17 14:23:25Z, [12:5] Done fit
2020-06-17 14:31:39Z, [12:6] Done fit
2020-06-17 14:40:23Z, [12:7] Done fit
2020-06-17 14:48:54Z, [12:8] Done fit
2020-06-17 14:57:47Z, [12:9] Done fit
2020-06-17 15:04:23Z, [12:10] Done fit
2020-06-17 15:19:31Z, [12] Done predict_proba
2020-06-17 15:19:31Z, [12] Done big_logloss, loss=0.0.
2020-06-17 15:19:31Z, [12] Done accuracy, acc=0.021199442119944213.
2020-06-17 15:19:31Z, [13:10] Done fit
2020-06-17 15:28:18Z, [13:0] Done fit
2020-06-17 15:37:06Z, [13:1] Done fit
2020-06-17 15:46:23Z, [13:2] Done fit
2020-06-17 15:55:24Z, [13:3] Done fit
2020-06-17 16:04:49Z, [13:4] Done fit
2020-06-17 16:13:59Z, [13:5] Done fit
2020-06-17 16:23:34Z, [13:6] Done fit
2020-06-17 16:32:50Z, [13:7] Done fit
2020-06-17 16:42:36Z, [13:8] Done fit
2020-06-17 16:52:04Z, [13:9] Done fit
2020-06-17 17:00:04Z, [13:10] Done fit
2020-06-17 17:17:24Z, [13] Done predict_proba
2020-06-17 17:17:24Z, [13] Done big_logloss, loss=0.0.
2020-06-17 17:17:24Z, [13] Done accuracy, acc=0.022594142259414227.
2020-06-17 17:17:24Z, [14:10] Done fit
```



```python
# Nothing mind blowing here.
plt.plot(range(14), acc_vec)
plt.xlabel('epoch')
plt.ylabel('acc')
```




    Text(0, 0.5, 'acc')




![png](2020-06-16_files/2020-06-16_10_1.png)


### 2020-06-18

#### that weird logloss ... 



```python
fu.big_logloss(y_test_enc, y_prob_vec, labels)    
```




    0.0




```python
y_test_enc.shape, y_prob_vec.shape, labels.shape
```




    ((3585,), (3585, 54), (54,))




```python
y_test_enc[:5]

```




    array([34, 45, 18, 41,  5])




```python
# Ahh ... so if all the values are super low, logloss fades?
print(np.argmax(y_prob_vec[:5, :], axis=1))
y_prob_vec[:5, :]
```

    [23  7 17  7 20]





    array([[3.43320407e-02, 1.45975398e-02, 1.37405249e-03, 3.63272801e-02,
            1.57198533e-02, 1.98668288e-03, 7.11898599e-03, 7.78052211e-02,
            9.97787993e-03, 1.23451920e-02, 2.27672025e-03, 5.64281363e-03,
            8.06225557e-03, 3.40091810e-03, 6.95233792e-03, 1.42584732e-02,
            5.61934011e-03, 9.11897793e-03, 1.73261724e-02, 2.23158412e-02,
            1.78813301e-02, 1.10056736e-01, 3.38553800e-03, 1.13068961e-01,
            1.50842301e-04, 4.20521423e-02, 7.04979850e-03, 1.20025994e-02,
            6.75320439e-03, 7.60487281e-04, 6.79432228e-03, 2.07565799e-02,
            7.43526891e-02, 6.23069294e-02, 4.86901738e-02, 3.42086470e-03,
            2.00877711e-02, 8.52537458e-04, 3.61542945e-04, 1.03140045e-02,
            2.09099017e-02, 2.03171559e-02, 2.66083190e-03, 1.87704358e-02,
            1.21015692e-02, 1.26861909e-03, 5.92931965e-03, 1.36819975e-02,
            9.47918184e-03, 1.92549676e-02, 7.43823545e-03, 2.45635223e-04,
            2.45275471e-04, 3.92340007e-05],
           [1.13724703e-02, 4.33668168e-03, 4.03582817e-04, 6.92225341e-03,
            1.39332777e-02, 1.61190820e-03, 2.09096842e-03, 2.83393562e-01,
            1.66558996e-02, 1.21449009e-02, 2.26105284e-03, 2.58109532e-03,
            5.44959726e-03, 2.11735209e-03, 5.09563554e-03, 1.24313263e-02,
            7.47357821e-03, 8.27851053e-03, 1.20807439e-02, 1.23237036e-02,
            2.18900815e-02, 3.52324881e-02, 4.85089328e-03, 6.39108196e-02,
            1.44102727e-03, 1.57984195e-03, 1.51892868e-03, 1.86148211e-02,
            1.07575553e-02, 6.51678303e-04, 2.20589489e-02, 2.00667128e-01,
            2.56775413e-02, 3.14233303e-02, 3.49258841e-03, 3.00011109e-03,
            5.90012316e-03, 5.39436005e-04, 3.09568888e-04, 3.02939955e-03,
            8.89371242e-03, 1.55405439e-02, 2.60572648e-03, 1.30862631e-02,
            2.64313109e-02, 1.07215543e-03, 4.39102668e-03, 1.69403516e-02,
            6.51075132e-03, 3.04438686e-07, 2.46124677e-02, 2.57302687e-04,
            1.33520109e-04, 2.01591065e-05],
           [1.00219222e-02, 5.82306460e-03, 1.18545606e-04, 9.62727237e-03,
            1.74595159e-03, 4.93757986e-03, 1.75451773e-04, 1.60150547e-02,
            2.92175030e-03, 1.79107208e-02, 3.83758987e-03, 3.24463705e-04,
            1.78701475e-01, 7.56689208e-03, 1.20573631e-02, 1.69108659e-02,
            1.32384067e-02, 3.77053291e-01, 2.63878852e-02, 5.67842275e-03,
            1.37230046e-02, 1.97000476e-03, 3.55870766e-03, 6.42400561e-03,
            4.73674620e-03, 1.61048747e-03, 4.84825636e-04, 1.72724240e-02,
            4.69100941e-03, 5.35734929e-03, 1.17457239e-02, 6.79301396e-02,
            9.28889867e-03, 3.11903120e-03, 2.03564006e-04, 5.01161558e-04,
            3.05468799e-03, 8.99023871e-05, 4.37883806e-04, 7.10516889e-03,
            2.13857173e-04, 4.73770825e-03, 9.35826625e-04, 2.34301295e-03,
            8.64369981e-03, 1.11553457e-03, 1.11323467e-03, 4.46197856e-03,
            3.86142085e-04, 7.99465701e-02, 2.11091097e-02, 2.22372037e-04,
            3.93820781e-04, 1.83790180e-05],
           [6.15562405e-03, 1.29264016e-02, 3.43316569e-05, 1.84753351e-02,
            3.68996477e-03, 3.79020785e-04, 2.67782481e-04, 2.20118806e-01,
            6.42478559e-03, 7.09167728e-03, 1.22226833e-04, 2.06250304e-04,
            5.14525454e-04, 8.10637954e-04, 4.99898242e-03, 7.97380880e-03,
            2.66266055e-02, 5.65080298e-03, 4.39017778e-03, 1.44837312e-02,
            3.19633521e-02, 9.35966223e-02, 8.54910258e-03, 8.89217854e-02,
            1.52460486e-02, 2.34667794e-03, 1.19347812e-03, 2.14530826e-02,
            2.05857004e-03, 4.24271071e-04, 4.88946512e-02, 7.23616686e-03,
            6.72372654e-02, 2.21090782e-02, 3.60832480e-03, 3.60246957e-03,
            7.59943016e-03, 4.70407685e-04, 2.31332844e-04, 3.01281326e-02,
            1.15565932e-03, 4.49882867e-03, 2.93037528e-03, 3.52278426e-02,
            3.82017009e-02, 5.27505018e-03, 2.20251386e-03, 4.30632345e-02,
            2.49090809e-02, 3.89365330e-02, 4.72581480e-03, 5.77592698e-04,
            8.27158656e-05, 1.33898641e-06],
           [5.33144362e-02, 8.74674600e-03, 4.70552547e-03, 1.82881486e-02,
            2.31490545e-02, 3.01193306e-03, 1.30525546e-03, 2.50697993e-02,
            1.84454303e-02, 1.49767136e-03, 1.64568797e-02, 2.07309797e-03,
            1.06080659e-02, 1.49900885e-03, 5.82267111e-03, 6.07391819e-02,
            4.25334387e-02, 5.67580089e-02, 5.10841459e-02, 3.16724591e-02,
            8.36721733e-02, 4.00946848e-02, 1.14471940e-02, 1.28054004e-02,
            6.31947676e-03, 4.25715744e-03, 1.77965776e-04, 1.90951396e-02,
            1.94816831e-02, 1.11032082e-02, 4.49019149e-02, 5.47672249e-02,
            1.67210102e-02, 1.22997165e-03, 2.03423835e-02, 8.61785095e-03,
            1.15226686e-03, 6.17770804e-03, 3.93093890e-03, 5.98935690e-03,
            1.11780781e-02, 8.42456613e-03, 1.58727134e-03, 2.75659505e-02,
            4.13099751e-02, 1.52806542e-03, 5.96439606e-03, 4.94650193e-02,
            1.48439817e-02, 2.23171376e-02, 6.10806374e-03, 3.60912643e-04,
            2.03361284e-04, 7.75708177e-05]], dtype=float32)




```python
# log_loss(y[:1], y_prob[:1], labels=labels); labe
a, b = np.array([0, 0, 0]), np.array([[0, 0., 0],
                                      [0., 0, 0],
                                      [0., 0, 0]])
print(log_loss(a, b, labels=['a', 'b', 'c']))

a, b = np.array(['a', 'a', 'a']), np.array([[0, 0., 0],
                                      [0., 0, 0],
                                      [0., 0, 0]])
print(log_loss(a, b, labels=['a', 'b', 'c']))

a, b = np.array(['a', 'a', 'a']), np.array([[1, 0., 0],
                                      [1., 0, 0],
                                      [1., 0, 0]])
print(log_loss(a, b, labels=['a', 'b', 'c']))

a, b = np.array(['a', 'a', 'a']), np.array([[0, 1., 0],
                                      [0., 1, 0],
                                      [1., 0, 0]])
print(log_loss(a, b, labels=['a', 'b', 'c']))

# Ok.. so whoops.. log loss was 0.0 because my y_true actually didnt correspond w/ the labels!!!
```

    0.0
    1.0986122886681098
    2.1094237467877998e-15
    23.025850929940457


    /opt/conda/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
      mask |= (ar1 == a)



```python
fu.big_logloss(y_test_enc, y_prob_vec, labels=list(range(54)))    
# makes sense now.
```




    4.921754919063358




```python
print(y_test_enc.shape, y_prob_vec.shape, labels.shape)

```

    (3585,) (3585, 54) (54,)



```python
os.getpid()
```




    689

### 2020-06-20

#### addendum about the classifier acc
* I had loaded this classifier and what the heck, thiis is weird
* The objective function is showing up as `'binary:logistic'` as opposed to `'multi:softprob'`, which is what happened with the `'/opt/program/artifacts/2020-06-11T041641Z/bundle.joblib'` classifier produced in [this notebook](https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-06-10-again.md) for instance , even though there I had also initialized the model w/o specifying the parameter , using `xgb.XGBClassifier()` . 

```python
# 2020-06-17T042025Z

model_loc = ('/opt/program/artifacts/2020-06-17T042025Z/model.xg')
#model = joblib.load(model_loc)
model = xgb.XGBClassifier()# .load_model(fname=model_loc)
model.load_model(fname=model_loc)

# model => 
XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, gamma=0,
              learning_rate=0.1, max_delta_step=0, max_depth=3,
              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,
              nthread=None, objective='binary:logistic', random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
              silent=None, subsample=1, verbosity=1)
```





