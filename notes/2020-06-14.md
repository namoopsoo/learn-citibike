

```python
# try again  the external memory approach w/ sklearn style XGboost...!
```


```python
import pandas as pd
import xgboost as xgb
from xgboost import XGBClassifier
import datetime; import pytz
import matplotlib as plt
from scipy.special import softmax
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split # (*arrays, **options)
import numpy as np
from sklearn.metrics import log_loss
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

from joblib import dump, load
import joblib
import os
from sklearn.metrics import confusion_matrix, mean_squared_error
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
import fresh.utils as fu

from importlib import reload
from collections import Counter
from tqdm.notebook import tqdm
import fresh.preproc.v1 as pv1
```


```python
datadir = '/opt/data'
localdir = '/opt/program'
tripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv')
stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',
                        index_col=0)

```


```python
artifactdir = '/opt/program/artifacts/2020-06-15T003722Z' 
bundle = joblib.load(f'{artifactdir}/bundle_2.joblib')
model = bundle['model']
dtest = xgb.DMatrix(f'{artifactdir}/dtest.txt')

y_preds = model.predict(dtest)
```

    [03:35:29] 168683x78 matrix with 168683 entries loaded from /opt/program/artifacts/2020-06-15T003722Z/dtest.txt



```python
bundle = joblib.load('/opt/program/artifacts/2020-06-11T041641Z/bundle.joblib')
model = bundle['model']
param = model.get_xgb_params()
print(param)
print(model.get_num_boosting_rounds()) 
# Oh hmm I was wondering in the "2020-06-12.ipynb" notebook why the results were so bad.
# Perhaps since this model appears to have `100` boosting rounds (in the past I suppse..) maybe indeed
# thiss is a matter of 10 vs 100 rounds.
```

    {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': nan, 'n_estimators': 100, 'nthread': 1, 'objective': 'multi:softprob', 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': 0, 'subsample': 1, 'verbosity': 1}
    100



```python
dtrain = xgb.DMatrix(f'{artifactdir}/dtrain.txt')
booster = xgb.train({**param, 'num_class': 54}, dtrain, num_boost_round=10)
```

    [03:46:07] 674733x78 matrix with 1349466 entries loaded from /opt/program/artifacts/2020-06-15T003722Z/dtrain.txt



```python
1
```




    1




```python
help(xgb.XGBClassifier)
# clf.fit(xgb_model=) # loaded before training (allows training continuation).
clf = xgb.XGBClassifier().load_model(fname='') # feature names not loaded 
# can also clf.save_model() # but they say in the doc... 
# **If you are using only the Python interface, we recommend pickling the
#      model object for best results.**
#

```

    Help on class XGBClassifier in module xgboost.sklearn:
    
    class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)
     |  XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)
     |  
     |  Implementation of the scikit-learn API for XGBoost classification.
     |  
     |  Parameters
     |  ----------
     |  max_depth : int
     |      Maximum tree depth for base learners.
     |  learning_rate : float
     |      Boosting learning rate (xgb's "eta")
     |  n_estimators : int
     |      Number of trees to fit.
     |  verbosity : int
     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).
     |  silent : boolean
     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.
     |  objective : string or callable
     |      Specify the learning task and the corresponding learning objective or
     |      a custom objective function to be used (see note below).
     |  booster: string
     |      Specify which booster to use: gbtree, gblinear or dart.
     |  nthread : int
     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)
     |  n_jobs : int
     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)
     |  gamma : float
     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.
     |  min_child_weight : int
     |      Minimum sum of instance weight(hessian) needed in a child.
     |  max_delta_step : int
     |      Maximum delta step we allow each tree's weight estimation to be.
     |  subsample : float
     |      Subsample ratio of the training instance.
     |  colsample_bytree : float
     |      Subsample ratio of columns when constructing each tree.
     |  colsample_bylevel : float
     |      Subsample ratio of columns for each level.
     |  colsample_bynode : float
     |      Subsample ratio of columns for each split.
     |  reg_alpha : float (xgb's alpha)
     |      L1 regularization term on weights
     |  reg_lambda : float (xgb's lambda)
     |      L2 regularization term on weights
     |  scale_pos_weight : float
     |      Balancing of positive and negative weights.
     |  base_score:
     |      The initial prediction score of all instances, global bias.
     |  seed : int
     |      Random number seed.  (Deprecated, please use random_state)
     |  random_state : int
     |      Random number seed.  (replaces seed)
     |  missing : float, optional
     |      Value in the data which needs to be present as a missing value. If
     |      None, defaults to np.nan.
     |  importance_type: string, default "gain"
     |      The feature importance type for the feature_importances_ property: either "gain",
     |      "weight", "cover", "total_gain" or "total_cover".
     |  \*\*kwargs : dict, optional
     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can
     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.
     |      Attempting to set a parameter via the constructor args and \*\*kwargs dict simultaneously
     |      will result in a TypeError.
     |  
     |      .. note:: \*\*kwargs unsupported by scikit-learn
     |  
     |          \*\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters
     |          passed via this argument will interact properly with scikit-learn.
     |  
     |  Note
     |  ----
     |  A custom objective function can be provided for the ``objective``
     |  parameter. In this case, it should have the signature
     |  ``objective(y_true, y_pred) -> grad, hess``:
     |  
     |  y_true: array_like of shape [n_samples]
     |      The target values
     |  y_pred: array_like of shape [n_samples]
     |      The predicted values
     |  
     |  grad: array_like of shape [n_samples]
     |      The value of the gradient for each sample point.
     |  hess: array_like of shape [n_samples]
     |      The value of the second derivative for each sample point
     |  
     |  Method resolution order:
     |      XGBClassifier
     |      XGBModel
     |      sklearn.base.BaseEstimator
     |      sklearn.base.ClassifierMixin
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  evals_result(self)
     |      Return the evaluation results.
     |      
     |      If **eval_set** is passed to the `fit` function, you can call
     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.
     |      When **eval_metric** is also passed to the `fit` function, the
     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.
     |      
     |      Returns
     |      -------
     |      evals_result : dictionary
     |      
     |      Example
     |      -------
     |      
     |      .. code-block:: python
     |      
     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}
     |      
     |          clf = xgb.XGBClassifier(**param_dist)
     |      
     |          clf.fit(X_train, y_train,
     |                  eval_set=[(X_train, y_train), (X_test, y_test)],
     |                  eval_metric='logloss',
     |                  verbose=True)
     |      
     |          evals_result = clf.evals_result()
     |      
     |      The variable **evals_result** will contain
     |      
     |      .. code-block:: python
     |      
     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},
     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}
     |  
     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)
     |      Fit gradient boosting classifier
     |      
     |      Parameters
     |      ----------
     |      X : array_like
     |          Feature matrix
     |      y : array_like
     |          Labels
     |      sample_weight : array_like
     |          Weight for each instance
     |      eval_set : list, optional
     |          A list of (X, y) pairs to use as a validation set for
     |          early-stopping
     |      sample_weight_eval_set : list, optional
     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of
     |          instance weights on the i-th validation set.
     |      eval_metric : str, callable, optional
     |          If a str, should be a built-in evaluation metric to use. See
     |          doc/parameter.rst. If callable, a custom evaluation metric. The call
     |          signature is func(y_predicted, y_true) where y_true will be a
     |          DMatrix object such that you may need to call the get_label
     |          method. It must return a str, value pair where the str is a name
     |          for the evaluation and value is the value of the evaluation
     |          function. This objective is always minimized.
     |      early_stopping_rounds : int, optional
     |          Activates early stopping. Validation error needs to decrease at
     |          least every <early_stopping_rounds> round(s) to continue training.
     |          Requires at least one item in evals. If there's more than one,
     |          will use the last. If early stopping occurs, the model will have
     |          three additional fields: bst.best_score, bst.best_iteration and
     |          bst.best_ntree_limit (bst.best_ntree_limit is the ntree_limit parameter
     |          default value in predict method if not any other value is specified).
     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree
     |          and/or num_class appears in the parameters)
     |      verbose : bool
     |          If `verbose` and an evaluation set is used, writes the evaluation
     |          metric measured on the validation set to stderr.
     |      xgb_model : str
     |          file name of stored xgb model or 'Booster' instance Xgb model to be
     |          loaded before training (allows training continuation).
     |      callbacks : list of callback functions
     |          List of callback functions that are applied at end of each iteration.
     |          It is possible to use predefined callbacks by using :ref:`callback_api`.
     |          Example:
     |      
     |          .. code-block:: python
     |      
     |              [xgb.callback.reset_learning_rate(custom_rates)]
     |  
     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)
     |      Predict with `data`.
     |      
     |      .. note:: This function is not thread safe.
     |      
     |        For each booster object, predict can only be called from one thread.
     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies
     |        of model object and then call ``predict()``.
     |      
     |      .. note:: Using ``predict()`` with DART booster
     |      
     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only
     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is
     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to
     |        a nonzero value, e.g.
     |      
     |        .. code-block:: python
     |      
     |          preds = bst.predict(dtest, ntree_limit=num_round)
     |      
     |      Parameters
     |      ----------
     |      data : DMatrix
     |          The dmatrix storing the input.
     |      output_margin : bool
     |          Whether to output the raw untransformed margin value.
     |      ntree_limit : int
     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined
     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).
     |      validate_features : bool
     |          When this is True, validate that the Booster's and data's feature_names are identical.
     |          Otherwise, it is assumed that the feature_names are the same.
     |      Returns
     |      -------
     |      prediction : numpy array
     |  
     |  predict_proba(self, data, ntree_limit=None, validate_features=True)
     |      Predict the probability of each `data` example being of a given class.
     |      
     |      .. note:: This function is not thread safe
     |      
     |          For each booster object, predict can only be called from one thread.
     |          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies
     |          of model object and then call predict
     |      
     |      Parameters
     |      ----------
     |      data : DMatrix
     |          The dmatrix storing the input.
     |      ntree_limit : int
     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined
     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).
     |      validate_features : bool
     |          When this is True, validate that the Booster's and data's feature_names are identical.
     |          Otherwise, it is assumed that the feature_names are the same.
     |      
     |      Returns
     |      -------
     |      prediction : numpy array
     |          a numpy array with the probability of each data example being of a given class.
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from XGBModel:
     |  
     |  __setstate__(self, state)
     |  
     |  apply(self, X, ntree_limit=0)
     |      Return the predicted leaf every tree for each sample.
     |      
     |      Parameters
     |      ----------
     |      X : array_like, shape=[n_samples, n_features]
     |          Input features matrix.
     |      
     |      ntree_limit : int
     |          Limit number of trees in the prediction; defaults to 0 (use all trees).
     |      
     |      Returns
     |      -------
     |      X_leaves : array_like, shape=[n_samples, n_trees]
     |          For each datapoint x in X and for each tree, return the index of the
     |          leaf x ends up in. Leaves are numbered within
     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.
     |  
     |  get_booster(self)
     |      Get the underlying xgboost Booster of this model.
     |      
     |      This will raise an exception when fit was not called
     |      
     |      Returns
     |      -------
     |      booster : a xgboost booster of underlying model
     |  
     |  get_num_boosting_rounds(self)
     |      Gets the number of xgboost boosting rounds.
     |  
     |  get_params(self, deep=False)
     |      Get parameters.
     |  
     |  get_xgb_params(self)
     |      Get xgboost type parameters.
     |  
     |  load_model(self, fname)
     |      Load the model from a file.
     |      
     |      The model is loaded from an XGBoost internal binary format which is
     |      universal among the various XGBoost interfaces. Auxiliary attributes of
     |      the Python Booster object (such as feature names) will not be loaded.
     |      Label encodings (text labels to numeric labels) will be also lost.
     |      **If you are using only the Python interface, we recommend pickling the
     |      model object for best results.**
     |      
     |      Parameters
     |      ----------
     |      fname : string or a memory buffer
     |          Input file name or memory buffer(see also save_raw)
     |  
     |  save_model(self, fname)
     |      Save the model to a file.
     |      
     |      The model is saved in an XGBoost internal binary format which is
     |      universal among the various XGBoost interfaces. Auxiliary attributes of
     |      the Python Booster object (such as feature names) will not be loaded.
     |      Label encodings (text labels to numeric labels) will be also lost.
     |      **If you are using only the Python interface, we recommend pickling the
     |      model object for best results.**
     |      
     |      Parameters
     |      ----------
     |      fname : string
     |          Output file name
     |  
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |      Modification of the sklearn method to allow unknown kwargs. This allows using
     |      the full range of xgboost parameters that are not defined as member variables
     |      in sklearn grid search.
     |      Returns
     |      -------
     |      self
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from XGBModel:
     |  
     |  coef_
     |      Coefficients property
     |      
     |      .. note:: Coefficients are defined only for linear learners
     |      
     |          Coefficients are only defined when the linear model is chosen as base
     |          learner (`booster=gblinear`). It is not defined for other base learner types, such
     |          as tree learners (`booster=gbtree`).
     |      
     |      Returns
     |      -------
     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``
     |  
     |  feature_importances_
     |      Feature importances property
     |      
     |      .. note:: Feature importance is defined only for tree boosters
     |      
     |          Feature importance is only defined when the decision tree model is chosen as base
     |          learner (`booster=gbtree`). It is not defined for other base learner types, such
     |          as linear learners (`booster=gblinear`).
     |      
     |      Returns
     |      -------
     |      feature_importances_ : array of shape ``[n_features]``
     |  
     |  intercept_
     |      Intercept (bias) property
     |      
     |      .. note:: Intercept is defined only for linear learners
     |      
     |          Intercept (bias) is only defined when the linear model is chosen as base
     |          learner (`booster=gblinear`). It is not defined for other base learner types, such
     |          as tree learners (`booster=gbtree`).
     |      
     |      Returns
     |      -------
     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |  
     |  __getstate__(self)
     |  
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.base.BaseEstimator:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.ClassifierMixin:
     |  
     |  score(self, X, y, sample_weight=None)
     |      Return the mean accuracy on the given test data and labels.
     |      
     |      In multi-label classification, this is the subset accuracy
     |      which is a harsh metric since you require for each sample that
     |      each label set be correctly predicted.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          Test samples.
     |      
     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)
     |          True labels for X.
     |      
     |      sample_weight : array-like of shape (n_samples,), default=None
     |          Sample weights.
     |      
     |      Returns
     |      -------
     |      score : float
     |          Mean accuracy of self.predict(X) wrt. y.
    



```python
X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)
X_train, X_test, y_train, y_test = train_test_split(X, y)

clf = xgb.XGBClassifier()


# preproc
(X_transformed,
     one_hot_enc, le,
     y_enc) = pv1.preprocess(X_train, y_train, # X[train_index]
                         neighborhoods)
labels = le.classes_

# Test set
X_test_transformed = one_hot_enc.transform(X_test)
y_test_enc = le.transform(y_test)
```


```python
len(Counter(y_train)) # 54
```




    54




```python
# pre-evaluate
# y_prob_vec = clf.predict_proba(X_test_transformed)
# loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
#
# ok, cannot call on raw model like TensorFlow. heh.
# XGBoostError: need to call fit or load_model beforehand
```


    ---------------------------------------------------------------------------

    XGBoostError                              Traceback (most recent call last)

    <ipython-input-13-d3a12c5ce30a> in <module>
          1 # pre-evaluate
    ----> 2 y_prob_vec = clf.predict_proba(X_test_transformed)
          3 loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)


    /opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py in predict_proba(self, data, ntree_limit, validate_features)
        830         if ntree_limit is None:
        831             ntree_limit = getattr(self, "best_ntree_limit", 0)
    --> 832         class_probs = self.get_booster().predict(test_dmatrix,
        833                                                  ntree_limit=ntree_limit,
        834                                                  validate_features=validate_features)


    /opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py in get_booster(self)
        191         """
        192         if self._Booster is None:
    --> 193             raise XGBoostError('need to call fit or load_model beforehand')
        194         return self._Booster
        195 


    XGBoostError: need to call fit or load_model beforehand



```python

```


```python
parts = fu.get_partitions(list(range(X_transformed.shape[0])), slice_size=1000)
loss_vec = []
for part in tqdm(parts):
    i, j = part[0], part[-1]   

    clf.fit(X_transformed[i:j], y_enc[i:j])
    
    y_prob_vec = clf.predict_proba(X_test_transformed)
    loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
    loss_vec.append(loss)
```


    HBox(children=(FloatProgress(value=0.0, max=633.0), HTML(value='')))


    



    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-17-499be10396ba> in <module>
          7 
          8     y_prob_vec = clf.predict_proba(X_test_transformed)
    ----> 9     loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
         10     loss_vec.append(loss)


    /opt/program/fresh/utils.py in big_logloss(y, y_prob, labels)
         63     for part in get_partitions(list(range(len(y_prob))), slice_size=1000):
         64         i, j = part[0], part[-1]
    ---> 65         losses_vec.append(log_loss(y[i:j], y_prob[i:j], labels=labels))
         66     return np.mean(losses_vec)
         67 


    /opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py in log_loss(y_true, y_pred, eps, normalize, sample_weight, labels)
       2289             raise ValueError('The number of classes in labels is different '
       2290                              'from that in y_pred. Classes found in '
    -> 2291                              'labels: {0}'.format(lb.classes_))
       2292 
       2293     # Renormalize


    ValueError: The number of classes in labels is different from that in y_pred. Classes found in labels: ['Alphabet City' 'Battery Park City' 'Bedford-Stuyvesant' 'Boerum Hill'
     'Bowery' 'Brooklyn Heights' 'Brooklyn Navy Yard' 'Central Park' 'Chelsea'
     'Chinatown' 'Civic Center' 'Clinton Hill'
     'Columbia Street Waterfront District' 'Downtown Brooklyn' 'Dumbo'
     'East Village' 'Financial District' 'Flatiron District' 'Fort Greene'
     'Fulton Ferry District' 'Garment District' 'Gramercy Park'
     'Greenwich Village' "Hell's Kitchen" 'Hudson Square' 'Kips Bay'
     'Korea Town' 'Lincoln Square' 'Little Italy' 'Lower East Side'
     'Lower Manhattan' 'Meatpacking District' 'Midtown' 'Midtown East'
     'Midtown West' 'Murray Hill' 'NoHo' 'NoMad' 'Nolita' 'Park Slope'
     'Peter Cooper Village' 'Rose Hill' 'SoHo' 'Stuyvesant Town'
     'Sutton Place' 'Theater District' 'Tribeca' 'Tudor City' 'Two Bridges'
     'Ukrainian Village' 'Union Square' 'Vinegar Hill' 'West Village'
     'Williamsburg']



```python
#loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
len(labels), y_test_enc.shape, y_prob_vec.shape, len(Counter(y_enc[i:j]))
```




    (54, (210854,), (210854, 53), 53)




```python
# Ok... going to try shuffling first instead.. 
# so the len(Counter(y_enc[i:j])) ,  hopefully , of the first fit , has 54 not 53.

# Ok I shuffled about four times until I ended up where the first part below here was '54'

size = X_transformed.shape[0]
print(size)
indices = np.random.choice(range(size), size=size, replace=False)
# X_train_shfl = X_transformed[indices]
# Ylabels_train_shfl = y_enc[indices]#.astype('int64')

parts = fu.get_partitions(indices, slice_size=1000)
len(parts[0])
len(Counter(y_enc[parts[0]]))
```

    632562





    54




```python
workdir = fu.make_work_dir()
```


```python
print("workdir", workdir); fu.log(workdir, 'First line')
```

    workdir /opt/program/artifacts/2020-06-16T033314Z



```python
fu.log(workdir, 'Starting')
loss_vec = []
for i, part in enumerate(tqdm(parts)):
    clf.fit(X_transformed[part], y_enc[part])
    fu.log(workdir, f'[{i}] Done fit')
    
    y_prob_vec = clf.predict_proba(X_test_transformed)
    fu.log(workdir, f'[{i}] Done predict_proba')
    
    loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
    fu.log(workdir, f'[{i}] Done big_logloss')
    
    loss_vec.append(loss)
```


    HBox(children=(FloatProgress(value=0.0, max=633.0), HTML(value='')))


    



    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-33-5be8c779daa1> in <module>
          8     fu.log(workdir, f'[{i}] Done predict_proba')
          9 
    ---> 10     loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
         11     fu.log(workdir, f'[{i}] Done big_logloss')
         12 


    /opt/program/fresh/utils.py in big_logloss(y, y_prob, labels)
         63     for part in get_partitions(list(range(len(y_prob))), slice_size=1000):
         64         i, j = part[0], part[-1]
    ---> 65         losses_vec.append(log_loss(y[i:j], y_prob[i:j], labels=labels))
         66     return np.mean(losses_vec)
         67 


    /opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py in log_loss(y_true, y_pred, eps, normalize, sample_weight, labels)
       2289             raise ValueError('The number of classes in labels is different '
       2290                              'from that in y_pred. Classes found in '
    -> 2291                              'labels: {0}'.format(lb.classes_))
       2292 
       2293     # Renormalize


    ValueError: The number of classes in labels is different from that in y_pred. Classes found in labels: ['Alphabet City' 'Battery Park City' 'Bedford-Stuyvesant' 'Boerum Hill'
     'Bowery' 'Brooklyn Heights' 'Brooklyn Navy Yard' 'Central Park' 'Chelsea'
     'Chinatown' 'Civic Center' 'Clinton Hill'
     'Columbia Street Waterfront District' 'Downtown Brooklyn' 'Dumbo'
     'East Village' 'Financial District' 'Flatiron District' 'Fort Greene'
     'Fulton Ferry District' 'Garment District' 'Gramercy Park'
     'Greenwich Village' "Hell's Kitchen" 'Hudson Square' 'Kips Bay'
     'Korea Town' 'Lincoln Square' 'Little Italy' 'Lower East Side'
     'Lower Manhattan' 'Meatpacking District' 'Midtown' 'Midtown East'
     'Midtown West' 'Murray Hill' 'NoHo' 'NoMad' 'Nolita' 'Park Slope'
     'Peter Cooper Village' 'Rose Hill' 'SoHo' 'Stuyvesant Town'
     'Sutton Place' 'Theater District' 'Tribeca' 'Tudor City' 'Two Bridges'
     'Ukrainian Village' 'Union Square' 'Vinegar Hill' 'West Village'
     'Williamsburg']



```python
# Observing here, this first log line from the file here , "2020-06-16 03:36:20Z, Starting" , 
# and I see that the big_logloss() first part  did not crash here. 
# ...   but loss vec ... hmm alll zeroes... not learning? 
```


```python
loss_vec
```




    [0.0, 0.0]




```python
y_test_enc.shape, y_prob_vec.shape, labels.shape
```




    ((210854,), (210854, 52), (54,))



##### Trying out that model save 


```python
# Since the output of y_prob_vec is showing a size of 52, this makes me think the fit() is starting from scratch
# ... I can at least quickly try out that save ...
```


```python
workdir = fu.make_work_dir(); print(workdir)
fu.log(workdir, 'Starting')
prev_model = None
loss_vec = []; acc_vec = []
for i, part in enumerate(tqdm(parts)):
    clf.fit(X_transformed[part], y_enc[part], xgb_model=prev_model)
    fu.log(workdir, f'[{i}] Done fit')
    
    prev_model = f'{workdir}/model.xg'
    clf.save_model(prev_model)
    
    y_prob_vec = clf.predict_proba(X_test_transformed)
    fu.log(workdir, f'[{i}] Done predict_proba')
    
    loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
    fu.log(workdir, f'[{i}] Done big_logloss, loss={loss}.')
    
    loss_vec.append(loss)
    
    acc = accuracy_score(y_test_enc, np.argmax(y_prob_vec, axis=1))
    acc_vec.append(acc)
    fu.log(workdir, f'[{i}] Done accuracy, acc={acc}.')
```

    /opt/program/artifacts/2020-06-16T035733Z



    HBox(children=(FloatProgress(value=0.0, max=633.0), HTML(value='')))


##### output logs
My logs, below, are showing the predict_proba part is what was taking the bulk of the time!


```
(pandars3) $ tail -f artifacts/2020-06-16T035733Z/work.log 
2020-06-16 03:57:33Z, Starting
2020-06-16 03:57:35Z, [0] Done fit
2020-06-16 03:59:10Z, [0] Done predict_proba
2020-06-16 03:59:10Z, [0] Done big_logloss, loss=0.0.
2020-06-16 03:59:10Z, [0] Done accuracy, acc=0.11610403407096853.
2020-06-16 03:59:13Z, [1] Done fit
2020-06-16 04:02:27Z, [1] Done predict_proba
2020-06-16 04:02:27Z, [1] Done big_logloss, loss=0.0.
2020-06-16 04:02:27Z, [1] Done accuracy, acc=0.11933849962533317.
2020-06-16 04:02:31Z, [2] Done fit
2020-06-16 04:09:21Z, [2] Done predict_proba
2020-06-16 04:09:21Z, [2] Done big_logloss, loss=0.0.
2020-06-16 04:09:21Z, [2] Done accuracy, acc=0.11234313790584954.
2020-06-16 04:09:27Z, [3] Done fit
2020-06-16 04:19:43Z, [3] Done predict_proba
2020-06-16 04:19:43Z, [3] Done big_logloss, loss=0.0.
2020-06-16 04:19:43Z, [3] Done accuracy, acc=0.0981295114154818.
2020-06-16 04:19:52Z, [4] Done fit
2020-06-16 04:35:20Z, [4] Done predict_proba
2020-06-16 04:35:20Z, [4] Done big_logloss, loss=0.0.
2020-06-16 04:35:20Z, [4] Done accuracy, acc=0.015944682102307758.
2020-06-16 04:35:31Z, [5] Done fit
2020-06-16 04:56:32Z, [5] Done predict_proba
2020-06-16 04:56:33Z, [5] Done big_logloss, loss=0.0.
2020-06-16 04:56:33Z, [5] Done accuracy, acc=0.10582678061597124.
2020-06-16 04:56:47Z, [6] Done fit
2020-06-16 05:22:41Z, [6] Done predict_proba
2020-06-16 05:22:41Z, [6] Done big_logloss, loss=0.0.
2020-06-16 05:22:41Z, [6] Done accuracy, acc=0.109895946958559.
2020-06-16 05:22:58Z, [7] Done fit
2020-06-16 05:53:38Z, [7] Done predict_proba
2020-06-16 05:53:38Z, [7] Done big_logloss, loss=0.0.
2020-06-16 05:53:38Z, [7] Done accuracy, acc=0.1033795896686807.
2020-06-16 05:53:58Z, [8] Done fit
2020-06-16 06:29:45Z, [8] Done predict_proba
2020-06-16 06:29:45Z, [8] Done big_logloss, loss=0.0.
2020-06-16 06:29:45Z, [8] Done accuracy, acc=0.020260464586870537.
2020-06-16 06:30:08Z, [9] Done fit
2020-06-16 07:10:49Z, [9] Done predict_proba
2020-06-16 07:10:49Z, [9] Done big_logloss, loss=0.0.
2020-06-16 07:10:49Z, [9] Done accuracy, acc=0.12078499815037894.
2020-06-16 07:11:14Z, [10] Done fit
2020-06-16 07:57:28Z, [10] Done predict_proba
2020-06-16 07:57:28Z, [10] Done big_logloss, loss=0.0.
2020-06-16 07:57:28Z, [10] Done accuracy, acc=0.12298557295569446.
2020-06-16 07:57:56Z, [11] Done fit
2020-06-16 08:49:22Z, [11] Done predict_proba
2020-06-16 08:49:23Z, [11] Done big_logloss, loss=0.0.
2020-06-16 08:49:23Z, [11] Done accuracy, acc=0.1153736708812733.
2020-06-16 08:49:54Z, [12] Done fit
2020-06-16 09:47:09Z, [12] Done predict_proba
2020-06-16 09:47:09Z, [12] Done big_logloss, loss=0.0.
2020-06-16 09:47:09Z, [12] Done accuracy, acc=0.11813861724226242.
2020-06-16 09:47:44Z, [13] Done fit
2020-06-16 10:50:05Z, [13] Done predict_proba
2020-06-16 10:50:06Z, [13] Done big_logloss, loss=0.0.
2020-06-16 10:50:06Z, [13] Done accuracy, acc=0.11920570631811585.
2020-06-16 10:50:45Z, [14] Done fit
2020-06-16 12:02:35Z, [14] Done predict_proba
2020-06-16 12:02:35Z, [14] Done big_logloss, loss=0.0.
2020-06-16 12:02:35Z, [14] Done accuracy, acc=0.11395088544680206.
2020-06-16 12:03:16Z, [15] Done fit
2020-06-16 13:22:31Z, [15] Done predict_proba
2020-06-16 13:22:31Z, [15] Done big_logloss, loss=0.0.
2020-06-16 13:22:31Z, [15] Done accuracy, acc=0.11543532491676706.
2020-06-16 13:23:15Z, [16] Done fit
2020-06-16 14:47:58Z, [16] Done predict_proba
2020-06-16 14:47:58Z, [16] Done big_logloss, loss=0.0.
2020-06-16 14:47:58Z, [16] Done accuracy, acc=0.026767336640519033.
2020-06-16 14:48:46Z, [17] Done fit
2020-06-16 16:19:31Z, [17] Done predict_proba
2020-06-16 16:19:32Z, [17] Done big_logloss, loss=0.0.
2020-06-16 16:19:32Z, [17] Done accuracy, acc=0.11969419598395098.
2020-06-16 16:20:23Z, [18] Done fit



```



```python

# still not sure why loss 0 tho. (EDIT: later learned this was because I was passing class name labels 
# and not encoded labels. Somehow instead of crashing that just produces a 0.0 log loss )
```
