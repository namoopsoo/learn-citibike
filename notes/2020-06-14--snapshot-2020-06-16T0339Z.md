

```python
# try again  the external memory approach w/ sklearn style XGboost...!
```


```python
import pandas as pd
import xgboost as xgb
from xgboost import XGBClassifier
import datetime; import pytz
import matplotlib as plt
from scipy.special import softmax
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split # (*arrays, **options)
import numpy as np
from sklearn.metrics import log_loss
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

from joblib import dump, load
import joblib
import os
from sklearn.metrics import confusion_matrix, mean_squared_error
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
import fresh.utils as fu

from importlib import reload
from collections import Counter
from tqdm.notebook import tqdm
import fresh.preproc.v1 as pv1
```


```python
datadir = '/opt/data'
localdir = '/opt/program'
tripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv')
stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',
                        index_col=0)

```


```python
artifactdir = '/opt/program/artifacts/2020-06-15T003722Z' 
bundle = joblib.load(f'{artifactdir}/bundle_2.joblib')
model = bundle['model']
dtest = xgb.DMatrix(f'{artifactdir}/dtest.txt')

y_preds = model.predict(dtest)
```

    [03:35:29] 168683x78 matrix with 168683 entries loaded from /opt/program/artifacts/2020-06-15T003722Z/dtest.txt



```python
bundle = joblib.load('/opt/program/artifacts/2020-06-11T041641Z/bundle.joblib')
model = bundle['model']
param = model.get_xgb_params()
print(param)
print(model.get_num_boosting_rounds()) 
# Oh hmm I was wondering in the "2020-06-12.ipynb" notebook why the results were so bad.
# Perhaps since this model appears to have `100` boosting rounds (in the past I suppse..) maybe indeed
# thiss is a matter of 10 vs 100 rounds.
```

    {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': nan, 'n_estimators': 100, 'nthread': 1, 'objective': 'multi:softprob', 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': 0, 'subsample': 1, 'verbosity': 1}
    100



```python
dtrain = xgb.DMatrix(f'{artifactdir}/dtrain.txt')
booster = xgb.train({**param, 'num_class': 54}, dtrain, num_boost_round=10)
```

    [03:46:07] 674733x78 matrix with 1349466 entries loaded from /opt/program/artifacts/2020-06-15T003722Z/dtrain.txt



```python
1
```




    1




```python
help(xgb.XGBClassifier)
# clf.fit(xgb_model=) # loaded before training (allows training continuation).
clf = xgb.XGBClassifier().load_model(fname='') # feature names not loaded 
# can also clf.save_model() # but they say in the doc... 
# **If you are using only the Python interface, we recommend pickling the
#      model object for best results.**
#

```


```python
X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)
X_train, X_test, y_train, y_test = train_test_split(X, y)

clf = xgb.XGBClassifier()


# preproc
(X_transformed,
     one_hot_enc, le,
     y_enc) = pv1.preprocess(X_train, y_train, # X[train_index]
                         neighborhoods)
labels = le.classes_

# Test set
X_test_transformed = one_hot_enc.transform(X_test)
y_test_enc = le.transform(y_test)
```


```python
len(Counter(y_train)) # 54
```




    54




```python
# pre-evaluate
# y_prob_vec = clf.predict_proba(X_test_transformed)
# loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
#
# ok, cannot call on raw model like TensorFlow. heh.
# XGBoostError: need to call fit or load_model beforehand
```


    ---------------------------------------------------------------------------

    XGBoostError                              Traceback (most recent call last)

    <ipython-input-13-d3a12c5ce30a> in <module>
          1 # pre-evaluate
    ----> 2 y_prob_vec = clf.predict_proba(X_test_transformed)
          3 loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)


    /opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py in predict_proba(self, data, ntree_limit, validate_features)
        830         if ntree_limit is None:
        831             ntree_limit = getattr(self, "best_ntree_limit", 0)
    --> 832         class_probs = self.get_booster().predict(test_dmatrix,
        833                                                  ntree_limit=ntree_limit,
        834                                                  validate_features=validate_features)


    /opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py in get_booster(self)
        191         """
        192         if self._Booster is None:
    --> 193             raise XGBoostError('need to call fit or load_model beforehand')
        194         return self._Booster
        195 


    XGBoostError: need to call fit or load_model beforehand



```python

```


```python
parts = fu.get_partitions(list(range(X_transformed.shape[0])), slice_size=1000)
loss_vec = []
for part in tqdm(parts):
    i, j = part[0], part[-1]   

    clf.fit(X_transformed[i:j], y_enc[i:j])
    
    y_prob_vec = clf.predict_proba(X_test_transformed)
    loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
    loss_vec.append(loss)
```


    HBox(children=(FloatProgress(value=0.0, max=633.0), HTML(value='')))


    



    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-17-499be10396ba> in <module>
          7 
          8     y_prob_vec = clf.predict_proba(X_test_transformed)
    ----> 9     loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
         10     loss_vec.append(loss)


    /opt/program/fresh/utils.py in big_logloss(y, y_prob, labels)
         63     for part in get_partitions(list(range(len(y_prob))), slice_size=1000):
         64         i, j = part[0], part[-1]
    ---> 65         losses_vec.append(log_loss(y[i:j], y_prob[i:j], labels=labels))
         66     return np.mean(losses_vec)
         67 


    /opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py in log_loss(y_true, y_pred, eps, normalize, sample_weight, labels)
       2289             raise ValueError('The number of classes in labels is different '
       2290                              'from that in y_pred. Classes found in '
    -> 2291                              'labels: {0}'.format(lb.classes_))
       2292 
       2293     # Renormalize


    ValueError: The number of classes in labels is different from that in y_pred. Classes found in labels: ['Alphabet City' 'Battery Park City' 'Bedford-Stuyvesant' 'Boerum Hill'
     'Bowery' 'Brooklyn Heights' 'Brooklyn Navy Yard' 'Central Park' 'Chelsea'
     'Chinatown' 'Civic Center' 'Clinton Hill'
     'Columbia Street Waterfront District' 'Downtown Brooklyn' 'Dumbo'
     'East Village' 'Financial District' 'Flatiron District' 'Fort Greene'
     'Fulton Ferry District' 'Garment District' 'Gramercy Park'
     'Greenwich Village' "Hell's Kitchen" 'Hudson Square' 'Kips Bay'
     'Korea Town' 'Lincoln Square' 'Little Italy' 'Lower East Side'
     'Lower Manhattan' 'Meatpacking District' 'Midtown' 'Midtown East'
     'Midtown West' 'Murray Hill' 'NoHo' 'NoMad' 'Nolita' 'Park Slope'
     'Peter Cooper Village' 'Rose Hill' 'SoHo' 'Stuyvesant Town'
     'Sutton Place' 'Theater District' 'Tribeca' 'Tudor City' 'Two Bridges'
     'Ukrainian Village' 'Union Square' 'Vinegar Hill' 'West Village'
     'Williamsburg']



```python
#loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
len(labels), y_test_enc.shape, y_prob_vec.shape, len(Counter(y_enc[i:j]))
```




    (54, (210854,), (210854, 53), 53)




```python
# Ok... going to try shuffling first instead.. 
# so the len(Counter(y_enc[i:j])) ,  hopefully , of the first fit , has 54 not 53.

# Ok I shuffled about four times until I ended up where the first part below here was '54'

size = X_transformed.shape[0]
print(size)
indices = np.random.choice(range(size), size=size, replace=False)
# X_train_shfl = X_transformed[indices]
# Ylabels_train_shfl = y_enc[indices]#.astype('int64')

parts = fu.get_partitions(indices, slice_size=1000)
len(parts[0])
len(Counter(y_enc[parts[0]]))
```

    632562





    54




```python
workdir = fu.make_work_dir()
```


```python
print("workdir", workdir); fu.log(workdir, 'First line')
```

    workdir /opt/program/artifacts/2020-06-16T033314Z



```python
fu.log(workdir, 'Starting')
loss_vec = []
for i, part in enumerate(tqdm(parts)):
    clf.fit(X_transformed[part], y_enc[part])
    fu.log(workdir, f'[{i}] Done fit')
    
    y_prob_vec = clf.predict_proba(X_test_transformed)
    fu.log(workdir, f'[{i}] Done predict_proba')
    
    loss = fu.big_logloss(y_test_enc, y_prob_vec, labels)
    fu.log(workdir, f'[{i}] Done big_logloss')
    
    loss_vec.append(loss)
```


    HBox(children=(FloatProgress(value=0.0, max=633.0), HTML(value='')))



```python

```
