
I want to do a quick pearson's chi squared independence test between the independent variables and dependent variable, just after still up to this point not producing much benefit from the last models. This should at least in a super rudimentary way help understand whether the variables are at all useful.




```python
import pandas as pd
import xgboost as xgb
from xgboost import XGBClassifier
import datetime; import pytz
import matplotlib.pyplot as plt
# from scipy.special import softmax
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split # (*arrays, **options)
import numpy as np
from sklearn.metrics import log_loss
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer
from sklearn.datasets import load_svmlight_file
from joblib import dump, load
import joblib
import os
from sklearn.metrics import confusion_matrix, mean_squared_error
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, balanced_accuracy_score
import fresh.utils as fu

from importlib import reload
from collections import Counter
from tqdm.notebook import tqdm
import fresh.preproc.v1 as pv1
import fresh.preproc.v2 as pv2
```


```python
localdir = '/home/ec2-user/SageMaker/learn-citibike'  # sagemaker
datadir = f'{localdir}/artifacts/2020-07-03T171842Z'

train_loc = f'{datadir}/train.libsvm'

# Convert the dtrain to numpy  ( nice advice from https://stackoverflow.com/a/40430328  )
train_data = load_svmlight_file(train_loc)
X_train = train_data[0].toarray()
y_train = train_data[1]
```


```python
rawdatadir = f'{localdir}/local_datas'

tripsdf = pd.read_csv(f'{rawdatadir}/2013-07 - Citi Bike trip data.csv'
                     )
stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',
                        index_col=0)
X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)

```


```python
# For my own sanity checking... 
# gender vs handedness , example from https://en.wikipedia.org/wiki/Contingency_table
# Also some inspiration on which statistic is good in my situation, 
#      from here , https://stackabuse.com/statistical-hypothesis-analysis-in-python-with-anovas-chi-square-and-pearson-correlation/
# one more reference that helped me : https://www.ling.upenn.edu/~clight/chisquared.htm
def chi_sq_test(table):
    chi2, p, dof, expected = chi2_contingency(table)
    print(f'chi2= {chi2}, p= {p}, dof= {dof}')
    # print(expected)
    print('Ho, our null hypothesis is the two variables are independent.')
    if p <= 0.05:
        print('p value <= 0.05 so can lean towards rejecting the null hypothesis.\n'
             '=> some dependency likely')
    else:
        print('p value > 0.05 so we likely cannot reject the null hypothesis \n'
             '=> independent')
    print('\n')
    
table = np.array([[43, 9], 
                  [44, 4]])
chi_sq_test(table)


table = np.array([[43, 40], 
                  [44, 4]])
chi_sq_test(table)

```

    chi2= 1.0724852071005921, p= 0.300384770390566, dof= 1
    Ho, our null hypothesis is the two variables are independent.
    p value > 0.05 so we likely cannot reject the null hypothesis 
    => independent
    
    
    chi2= 19.911071844194957, p= 8.11291182934436e-06, dof= 1
    Ho, our null hypothesis is the two variables are independent.
    p value <= 0.05 so can lean towards rejecting the null hypothesis.
    => some dependency likely
    
    



```python
# Some quick playing around first ...  using column 2 which is the "time of day" feature.
indices = np.random.choice(range(100000), replace=False, size=10000)
# X[0] # array(['Midtown East', 0, 4, 'Customer', 1], dtype=object)
table = pd.crosstab(X[indices, 2], y[indices])

#X[indices, 4]#.shape, y[indices].shape
#Counter(X[indices, 2])
# from scipy.stats import chi2_contingency

chi2, p, dof, expected = chi2_contingency(table)
print(f'chi2 {chi2}, p {p}, dof {dof}')
print(table)
print(expected)
```

    chi2 960.8880653984245, p 5.473221121152702e-165, dof 56
    col_0  Chelsea  East Village  Garment District  Gramercy Park  \
    row_0                                                           
    0           69           123                25             49   
    1           36           128                49             44   
    2           89           118                83             42   
    3           84           268               109            105   
    4            9            61                28             38   
    
    col_0  Lower East Side  Midtown  Midtown East  Midtown West  Murray Hill  \
    row_0                                                                      
    0                   12      136           612           407          235   
    1                   22      198           318           282          144   
    2                   20      206           296           199          112   
    3                   70      175           536           264          261   
    4                   28       34           130            48           63   
    
    col_0  Nolita  Rose Hill  Stuyvesant Town  Theater District  \
    row_0                                                         
    0          88         64               51               342   
    1         130         54               89               160   
    2         126         51               96               132   
    3         188        116              314               219   
    4          69         32               84                78   
    
    col_0  Ukrainian Village  Union Square  
    row_0                                   
    0                     14           154  
    1                     44           171  
    2                     80           170  
    3                    189           248  
    4                     46            36  
    [[ 68.3347 166.1938  70.0014  66.1918  36.1912 178.3369 450.4852 285.72
      194.0515 143.0981  75.4777 150.9554 221.6711  88.8113 185.4799]
     [ 53.6403 130.4562  54.9486  51.9582  28.4088 139.9881 353.6148 224.28
      152.3235 112.3269  59.2473 118.4946 174.0039  69.7137 145.5951]
     [ 52.234  127.036   53.508   50.596   27.664  136.318  344.344  218.4
      148.33   109.382   57.694  115.388  169.442   67.886  141.778 ]
     [ 90.2902 219.5908  92.4924  87.4588  47.8192 235.6354 595.2232 377.52
      256.399  189.0746  99.7282 199.4564 292.8926 117.3458 245.0734]
     [ 22.5008  54.7232  23.0496  21.7952  11.9168  58.7216 148.3328  94.08
       63.896   47.1184  24.8528  49.7056  72.9904  29.2432  61.0736]]



```python
table
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>col_0</th>
      <th>Chelsea</th>
      <th>East Village</th>
      <th>Garment District</th>
      <th>Gramercy Park</th>
      <th>Lower East Side</th>
      <th>Midtown</th>
      <th>Midtown East</th>
      <th>Midtown West</th>
      <th>Murray Hill</th>
      <th>Nolita</th>
      <th>Rose Hill</th>
      <th>Stuyvesant Town</th>
      <th>Theater District</th>
      <th>Ukrainian Village</th>
      <th>Union Square</th>
    </tr>
    <tr>
      <th>row_0</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>59</td>
      <td>124</td>
      <td>23</td>
      <td>51</td>
      <td>11</td>
      <td>161</td>
      <td>594</td>
      <td>435</td>
      <td>214</td>
      <td>104</td>
      <td>65</td>
      <td>48</td>
      <td>387</td>
      <td>16</td>
      <td>166</td>
    </tr>
    <tr>
      <th>1</th>
      <td>40</td>
      <td>112</td>
      <td>40</td>
      <td>50</td>
      <td>20</td>
      <td>178</td>
      <td>348</td>
      <td>238</td>
      <td>143</td>
      <td>103</td>
      <td>59</td>
      <td>96</td>
      <td>169</td>
      <td>59</td>
      <td>194</td>
    </tr>
    <tr>
      <th>2</th>
      <td>78</td>
      <td>120</td>
      <td>71</td>
      <td>68</td>
      <td>28</td>
      <td>161</td>
      <td>294</td>
      <td>196</td>
      <td>113</td>
      <td>106</td>
      <td>54</td>
      <td>98</td>
      <td>153</td>
      <td>70</td>
      <td>171</td>
    </tr>
    <tr>
      <th>3</th>
      <td>101</td>
      <td>248</td>
      <td>114</td>
      <td>117</td>
      <td>80</td>
      <td>185</td>
      <td>511</td>
      <td>258</td>
      <td>240</td>
      <td>209</td>
      <td>124</td>
      <td>304</td>
      <td>207</td>
      <td>188</td>
      <td>257</td>
    </tr>
    <tr>
      <th>4</th>
      <td>9</td>
      <td>64</td>
      <td>34</td>
      <td>34</td>
      <td>26</td>
      <td>30</td>
      <td>122</td>
      <td>70</td>
      <td>61</td>
      <td>57</td>
      <td>33</td>
      <td>81</td>
      <td>64</td>
      <td>42</td>
      <td>42</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Ok.. got for all of it... compare independent variable  at a time.. 
# indices = np.random.choice(range(100000), replace=False, size=10000)
# X[0] # array(['Midtown East', 0, 4, 'Customer', 1], dtype=object)
independent_variables = {0: 'start_neighborhood',
                        1: 'gender',
                        2: 'time_of_day',
                        3: 'usertype',
                        4: 'weekday'}
for col_index, var in independent_variables.items():
    # Build a cross-tab 
    print('====================================================')
    print(f'Building crosstab on {var}')
    table = pd.crosstab(X[:, col_index], y)
    #print(table)
    chi_sq_test(table)
    #print()


```

    ====================================================
    Building crosstab on start_neighborhood
    chi2= 1101939.4755814166, p= 0.0, dof= 2809
    Ho, our null hypothesis is the two variables are independent.
    p value <= 0.05 so can lean towards rejecting the null hypothesis.
    => some dependency likely
    
    
    ====================================================
    Building crosstab on gender
    chi2= 28181.371173591728, p= 0.0, dof= 106
    Ho, our null hypothesis is the two variables are independent.
    p value <= 0.05 so can lean towards rejecting the null hypothesis.
    => some dependency likely
    
    
    ====================================================
    Building crosstab on time_of_day
    chi2= 43677.30494407689, p= 0.0, dof= 212
    Ho, our null hypothesis is the two variables are independent.
    p value <= 0.05 so can lean towards rejecting the null hypothesis.
    => some dependency likely
    
    
    ====================================================
    Building crosstab on usertype
    chi2= 23638.087048050013, p= 0.0, dof= 53
    Ho, our null hypothesis is the two variables are independent.
    p value <= 0.05 so can lean towards rejecting the null hypothesis.
    => some dependency likely
    
    
    ====================================================
    Building crosstab on weekday
    chi2= 9183.135943620315, p= 0.0, dof= 53
    Ho, our null hypothesis is the two variables are independent.
    p value <= 0.05 so can lean towards rejecting the null hypothesis.
    => some dependency likely
    
    


There can be much more to be said and done here, but for now I think I will casually use this as proof that I should keep going ! There's yet a model that can capture the entropy in this data!.


```python

```
