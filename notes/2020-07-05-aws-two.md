
#### What
random split instead of my rebalance ... just for comparison

Fork of the "2020-07-03-aws" notebook basically... but randomly shrink by `50%` instead of my rebalancing func..

#### Summary
- Wow comparing w/ the slightly worse accuracy in "2020-07-03-aws" is quite meaningful, because the balanced accuracy was really close to the raw accuracy in "2020-07-03-aws", since that is the notebook where I tried "balancing out" the training data, that makes sense.
- But in this notebook, as a proof of concept I only shrunk the training set randomly (for memory reasons) , without balancing , and in this notebook, the balanced error was `5%` worse than the raw unweighted accuracy.
- So definitely the training set label balancing was doing the right thing and I think it was a good practice indeed as always. 
- That being said I think my first version of balancing was not super great so I could do a better job and give this another go. My first version of dataset balancing brought down the dominant class too much.
- The visualization of the confusion matrix looks pretty damning here too, compared to that in the "2020-07-03-aws" notebook. 


```python
import pandas as pd
import xgboost as xgb
from xgboost import XGBClassifier
import datetime; import pytz
import matplotlib.pyplot as plt
# from scipy.special import softmax
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split # (*arrays, **options)
import numpy as np
from sklearn.metrics import log_loss
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer
from sklearn.datasets import load_svmlight_file
from joblib import dump, load
import joblib
import os
from sklearn.metrics import confusion_matrix, mean_squared_error
from sklearn.model_selection import KFold, train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, balanced_accuracy_score
import fresh.utils as fu

from importlib import reload
from collections import Counter
from tqdm.notebook import tqdm
import fresh.preproc.v1 as pv1
import fresh.preproc.v2 as pv2
```


```python
# localdir = '/opt/program'  # laptop docker 
localdir = '/home/ec2-user/SageMaker/learn-citibike'  # sagemaker
datadir = f'{localdir}/local_datas'

tripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv'
                     )#.sample(frac=0.017, random_state=42)
stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',
                        index_col=0)
X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)
fu.get_my_memory()
```




    {'pmem': '14.9', 'rss': '0.575 GiB'}




```python
indices = np.random.choice(range(X.shape[0]), replace=False, size=X.shape[0]//2)
print(X.shape, X.shape[0]//2, indices.shape)
```

    (843416, 5) 421708 (421708,)



```python
# Shrink
X, y = X[indices], y[indices]
```


```python
fu.get_my_memory()
workdir = fu.make_work_dir(localdir); print(workdir)
fu.log(workdir, 'new workdir')
```

    /home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z



```python
%%time
# Should really make this into a func... 
# (for now just copying my work from https://github.com/namoopsoo/learn-citibike/blob/2020-revisit/notes/2020-07-03-aws.md directly)

X_train, X_test, y_train, y_test = train_test_split(X, y)
proc_bundle, train_loc = pv2.preprocess(
        X_train, y_train, neighborhoods, workdir=workdir,
        dataset_name='train')
print(train_loc)
bundle_loc = f'{workdir}/proc_bundle.joblib'
joblib.dump({'notebook': '2020-07-03-aws.ipynb',
            'proc_bundle': proc_bundle,
            },
           f'{workdir}/proc_bundle.joblib')
print('Done ', bundle_loc)
test_loc = pv2.preprocess(
        X_test, y_test, neighborhoods, proc_bundle=proc_bundle,
        workdir=workdir,
        dataset_name='test')
print('Done ', test_loc)
```

    100%|██████████| 11/11 [00:11<00:00,  1.02s/it]
      0%|          | 0/11 [00:00<?, ?it/s]

    /home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z/train.libsvm
    Done  /home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z/proc_bundle.joblib


    100%|██████████| 11/11 [00:03<00:00,  2.91it/s]

    Done  /home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z/test.libsvm
    CPU times: user 17 s, sys: 625 ms, total: 17.6 s
    Wall time: 15.4 s


    



```python
# more cleanup..
print(fu.get_my_memory())
del X, y, X_train, X_test, y_train, y_test
print(fu.get_my_memory())
```

    {'pmem': '12.2', 'rss': '0.471 GiB'}
    {'pmem': '9.7', 'rss': '0.374 GiB'}



```python
# ok ... load w/o caching... 
train_loc = f'{workdir}/train.libsvm'
dtrain = xgb.DMatrix(
    f'{train_loc}?format=libsvm')
```

    [00:00:53] 316281x85 matrix with 1438050 entries loaded from /home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z/train.libsvm?format=libsvm



```python
%%time
# ok try the parameters from "2020-06-28-take2.ipynb"  again...

params = {'max_depth':3, 
          'learning_rate': .1, # 'eta':0.1   # alias
          'objective':'multi:softprob',   # mlogloss? 
          'num_class':  54 ,
          'base_score':0.5, 
          'booster':'gbtree', 
          'colsample_bylevel':1,
          'colsample_bynode':1, 
          'colsample_bytree':1, 
          'gamma':0,
          'max_delta_step':0, 
          'min_child_weight':1, #'missing':nan, 
          'random_state':0,
          'reg_alpha':0, 
          'reg_lambda':1,
          'scale_pos_weight':1, 
          'seed': 42,
          #'silent':None, 
          'subsample':1, 
          'verbosity': 2
          
          # from sklearn...
          # 'n_estimators':100, 'n_jobs':1,
         }

watchlist = [(dtrain, 'train'), 
             #(dtest, 'test')
            ]
num_round = 100
fu.log(workdir, 'Start xgb.train')
xgb_model = xgb.train(params, dtrain, num_round, watchlist)
```

    [00:01:09] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3
    [00:01:09] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3
    [1]	train-merror:0.845985
    ...
    [23]	train-merror:0.843898
    [00:02:18] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3
    ...
    [49]	train-merror:0.841018
    [00:03:29] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3
    [00:03:29] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3
    ...
    [00:05:37] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3
    [00:05:37] INFO: src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3
    [99]	train-merror:0.837278
    CPU times: user 8min 33s, sys: 1.89 s, total: 8min 35s
    Wall time: 4min 28s



```python
test_loc = f'{workdir}/test.libsvm'
dtest = xgb.DMatrix(f'{test_loc}?format=libsvm')
fu.log(workdir, f'Start predict()', f'mem, ({fu.get_my_memory()})')
y_prob_vec = xgb_model.predict(dtest)
predictions = np.argmax(y_prob_vec, axis=1)
fu.log(workdir, f'Done predict()', f'mem, ({fu.get_my_memory()})')

actuals = dtest.get_label()

logloss = fu.big_logloss(actuals, y_prob=y_prob_vec, 
                         labels= list(range(54)))
fu.log(workdir, f'Done  done fu.big_logloss() logloss={logloss}',
              f'mem, ({fu.get_my_memory()})')
print('logloss', logloss)
acc = accuracy_score(actuals, predictions)
print('acc', acc)
balanced_acc = balanced_accuracy_score(actuals, predictions)
print('balanced acc', balanced_acc)
```

    [00:06:33] 105427x85 matrix with 478828 entries loaded from /home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z/test.libsvm?format=libsvm
    logloss 3.282076793024198
    acc 0.15964601098390355
    balanced acc 0.08281646671786597



```python
confusion = confusion_matrix(actuals, predictions)

plt.figure(figsize=(10,10))
plt.imshow(confusion)
plt.colorbar()
plt.grid(False)
plt.show()
```


![png](2020-07-05-aws-two_files/2020-07-05-aws-two_11_0.png)



```python
bundle_loc = f'{workdir}/bundle_with_metrics.joblib'
print('Saving to ', bundle_loc)

joblib.dump({
    'notebook': '2020-07-05-aws-two.ipynb',
    'xgb_model': xgb_model,
    'train':  '/home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z/train.libsvm',
    'walltime':  'Wall time: 4min 28s',
    'primary_dataset': '2013-07 - Citi Bike trip data.csv',
    'input_params': params,
    'num_round': num_round,
    'proc_bundle':  
    '/home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z/proc_bundle.joblib',
    'validation_metrics': {
        'accuracy': acc,
        'balanced_accuracy': balanced_acc,
        'confusion': confusion,
        'logloss': logloss,
        'test':  'artifacts/2020-07-05T235821Z/test.libsvm?format=libsvm',
    },
    # 'train_metrics': {'train_accuracy': }
}, bundle_loc
)
```

    Saving to  /home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z/bundle_with_metrics.joblib





    ['/home/ec2-user/SageMaker/learn-citibike/artifacts/2020-07-05T235821Z/bundle_with_metrics.joblib']




```python
# Train acc?
print('calc training set acc')
y_prob_vec = xgb_model.predict(dtrain)
predictions = np.argmax(y_prob_vec, axis=1)
actuals = dtrain.get_label()

logloss = fu.big_logloss(actuals, y_prob=y_prob_vec, 
                         labels= list(range(54)))
print('logloss', logloss)
acc = accuracy_score(actuals, predictions)
print('acc', acc)
balanced_acc = balanced_accuracy_score(actuals, predictions)
print('balanced acc', balanced_acc)


```

    calc training set acc
    logloss 3.2659223088639417
    acc 0.16272238926777138
    balanced acc 0.08468386309315597


Hmm then unclear what is the `[99]	train-merror:0.837278`  that was listed on the last epoch,
during the training... kind of weird since i'm getting a different number.

Ah wait nevermind... it's just 1-acc.. makes sense now duh.
