{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- I had tried building a new dataset in \"2020-06-24\" but kernel kept on exploding. \n",
    "- So here, first I tried iterating through chunks of preprocessing at a time. That still exploded the kernel\n",
    "- But then [midway](#2020-06-27) I tried out a really cool numpy feature where you can save an array to a file in append mode. \n",
    "- And the preprocessing step (using encoders to build transformed data and saving that to disk) , on the `843,416` rows here, took about `39` seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import datetime; import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split # (*arrays, **options)\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from joblib import dump, load\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import fresh.utils as fu\n",
    "\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import fresh.preproc.v1 as pv1\n",
    "import fresh.preproc.v2 as pv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/opt/data'\n",
    "localdir = '/opt/program'\n",
    "\n",
    "\n",
    "tripsdf = pd.read_csv(f'{datadir}/2013-07 - Citi Bike trip data.csv'\n",
    "                     )#.sample(frac=0.017, random_state=42)\n",
    "stationsdf = pd.read_csv(f'{localdir}/datas/stations/stations-2018-12-04-c.csv',\n",
    "                        index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y, neighborhoods = fu.prepare_data(tripsdf, stationsdf)\n",
    "# \n",
    "# future thinking here...\n",
    "# ..disk approach => return the location of X, y on disk instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/program/artifacts/2020-06-26T142154Z\n"
     ]
    }
   ],
   "source": [
    "workdir = fu.make_work_dir(); print(workdir)\n",
    "fu.log(workdir, 'Starting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 359.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# place a small section of the pre-processed data into a target file..\n",
    "x_outfile = f'{workdir}/x_transformed.csv'\n",
    "y_outfile = f'{workdir}/y_enc.csv'\n",
    "\n",
    "X_transformed, y_enc, proc_dict = pv2.preprocess(\n",
    "        X[:1000], y[:1000], neighborhoods, labeled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(x_outfile, X_transformed, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 85), (1000,), (1000, 86))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_transformed.shape, y_enc.shape, \n",
    " np.hstack((np.resize(y_enc, (1000, 1)), X_transformed)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_outfile = f'{workdir}/minidata.csv'\n",
    "yx_data = np.hstack((np.resize(y_enc, (1000, 1)), X_transformed))\n",
    "np.savetxt(both_outfile, yx_data, delimiter=',', fmt='%u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020-06-27\n",
    "\n",
    "#### trying a looped numpy append technique\n",
    "* Read [here](https://stackoverflow.com/questions/27786868/python3-numpy-appending-to-a-file-using-numpy-savetxt#27980725) that you can pass a file description to `np.savetxt` to accomplish appending. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just writing same data a few times first..\n",
    "both_outfile = f'{workdir}/minidata.csv'\n",
    "yx_data = np.hstack((np.resize(y_enc, (1000, 1)), X_transformed))\n",
    "with open(both_outfile, 'ab') as fd:\n",
    "    np.savetxt(fd, yx_data, delimiter=',', fmt='%u')\n",
    "    np.savetxt(fd, yx_data, delimiter=',', fmt='%u')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 86)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.loadtxt(both_outfile, delimiter=',')\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/program/artifacts/2020-06-26T142154Z/data.csv'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nice!!! that worked .\n",
    "dataset_name = None#'train'\n",
    "outfile = f'{workdir}/{dataset_name or \"data\"}.csv'\n",
    "outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fresh.preproc.v2' from '/opt/program/fresh/preproc/v2.py'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(pv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:38<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.39 s, sys: 3.74 s, total: 11.1 s\n",
      "Wall time: 39.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with ipdb.launch_ipdb_on_exception():\n",
    "    proc_bundle, outfile = pv2.preprocess(\n",
    "        X, y, neighborhoods, workdir=workdir,\n",
    "        dataset_name='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok cool that appears to have worked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'enc': OneHotEncoder(categories=[['Alphabet City', 'Battery Park City',\n",
       "                             'Bedford-Stuyvesant', 'Bloomingdale', 'Boerum Hill',\n",
       "                             'Bowery', 'Broadway Triangle', 'Brooklyn Heights',\n",
       "                             'Brooklyn Navy Yard', 'Carnegie Hill',\n",
       "                             'Carroll Gardens', 'Central Park', 'Chelsea',\n",
       "                             'Chinatown', 'Civic Center', 'Clinton Hill',\n",
       "                             'Cobble Hill', 'Columbia Street Waterfront District',\n",
       "                             'Downtown Brooklyn', 'Dumbo', 'East Harlem',\n",
       "                             'East Village', 'East Williamsburg',\n",
       "                             'Financial District', 'Flatiron District',\n",
       "                             'Fort Greene', 'Fulton Ferry District',\n",
       "                             'Garment District', 'Governors Island', 'Gowanus', ...],\n",
       "                            [0, 1, 2], [0, 1, 2, 3, 4]],\n",
       "                drop=None, dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "                sparse=True),\n",
       "  'usertype_le': LabelEncoder(),\n",
       "  'le': LabelEncoder()},\n",
       " '/opt/program/artifacts/2020-06-26T142154Z/train.csv')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_bundle, outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 139M Jun 27 18:38 /opt/program/artifacts/2020-06-26T142154Z/train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh '/opt/program/artifacts/2020-06-26T142154Z/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 86)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yx_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/program/artifacts/2020-06-26T142154Z/minidata.libsvm'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_outfile = f'{workdir}/minidata.libsvm'\n",
    "with open(both_outfile, 'ab') as fd:\n",
    "    sd.dump_svmlight_file(yx_data[:100, 1:], yx_data[:100, 0], fd)\n",
    "    sd.dump_svmlight_file(yx_data[100:200, 1:], yx_data[100:200, 0], fd)\n",
    "    sd.dump_svmlight_file(yx_data[200:300, 1:], yx_data[200:300, 0], fd)\n",
    "both_outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:46<00:00,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 s, sys: 370 ms, total: 21.9 s\n",
      "Wall time: 47.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/opt/program/artifacts/2020-06-26T142154Z/train.libsvm'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "reload(pv2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "proc_bundle, outfile = pv2.preprocess(\n",
    "        X_train, y_train, neighborhoods, workdir=workdir,\n",
    "        dataset_name='train')\n",
    "outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:28:46] 1000x85 matrix with 85000 entries loaded from /opt/program/artifacts/2020-06-26T142154Z/minidata.csv?format=csv&label_column=0&delimiter=,\n"
     ]
    }
   ],
   "source": [
    "dmatrix = xgb.DMatrix(\n",
    "    f'{both_outfile}?format=csv&label_column=0&delimiter=,')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-aee50f0fc1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multi:softprob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmatrix\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective='multi:softprob')\n",
    "xgb_model.fit(dmatrix,  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### thoughts on params to mess with..\n",
    "num_round , make sure at least 100\n",
    "gamma, 0\n",
    "max_delta_step , 1\n",
    "n_estimators , >100..\n",
    "min_child_weight, 30\n",
    "max_depth , 3,4,5,6...\n",
    "colsample_bytree, 0.4..1.0 \n",
    "subsample, 0.5..1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### references\n",
    "- [this](https://github.com/aws/sagemaker-xgboost-container/blob/master/src/sagemaker_xgboost_container/data_utils.py) is a handy reference around xgb utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1342"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getpid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
